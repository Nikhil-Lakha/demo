{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01786,
     "end_time": "2020-10-07T10:31:41.681660",
     "exception": false,
     "start_time": "2020-10-07T10:31:41.663800",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-10-07T10:31:41.729682Z",
     "iopub.status.busy": "2020-10-07T10:31:41.728858Z",
     "iopub.status.idle": "2020-10-07T10:31:43.774264Z",
     "shell.execute_reply": "2020-10-07T10:31:43.773550Z"
    },
    "papermill": {
     "duration": 2.075554,
     "end_time": "2020-10-07T10:31:43.774414",
     "exception": false,
     "start_time": "2020-10-07T10:31:41.698860",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import re\n",
    "re.compile('<title>(.*)</title>')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016583,
     "end_time": "2020-10-07T10:31:43.808336",
     "exception": false,
     "start_time": "2020-10-07T10:31:43.791753",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-07T10:31:43.857364Z",
     "iopub.status.busy": "2020-10-07T10:31:43.856589Z",
     "iopub.status.idle": "2020-10-07T10:31:43.958708Z",
     "shell.execute_reply": "2020-10-07T10:31:43.958045Z"
    },
    "papermill": {
     "duration": 0.132995,
     "end_time": "2020-10-07T10:31:43.958846",
     "exception": false,
     "start_time": "2020-10-07T10:31:43.825851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-07T10:31:44.013821Z",
     "iopub.status.busy": "2020-10-07T10:31:44.012878Z",
     "iopub.status.idle": "2020-10-07T10:31:44.017979Z",
     "shell.execute_reply": "2020-10-07T10:31:44.017352Z"
    },
    "papermill": {
     "duration": 0.040383,
     "end_time": "2020-10-07T10:31:44.018123",
     "exception": false,
     "start_time": "2020-10-07T10:31:43.977740",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    8530\n",
       " 2    3640\n",
       " 0    2353\n",
       "-1    1296\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15814</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @ezlusztig: They took down the material on ...</td>\n",
       "      <td>22001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15815</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @washingtonpost: How climate change could b...</td>\n",
       "      <td>17856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15816</th>\n",
       "      <td>0</td>\n",
       "      <td>notiven: RT: nytimesworld :What does Trump act...</td>\n",
       "      <td>384248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15817</th>\n",
       "      <td>-1</td>\n",
       "      <td>RT @sara8smiles: Hey liberals the climate chan...</td>\n",
       "      <td>819732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15818</th>\n",
       "      <td>0</td>\n",
       "      <td>RT @Chet_Cannon: .@kurteichenwald's 'climate c...</td>\n",
       "      <td>806319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentiment                                            message  tweetid\n",
       "15814          1  RT @ezlusztig: They took down the material on ...    22001\n",
       "15815          2  RT @washingtonpost: How climate change could b...    17856\n",
       "15816          0  notiven: RT: nytimesworld :What does Trump act...   384248\n",
       "15817         -1  RT @sara8smiles: Hey liberals the climate chan...   819732\n",
       "15818          0  RT @Chet_Cannon: .@kurteichenwald's 'climate c...   806319"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to clean out any noise from the data\n",
    "def standardize_text(df, text_field):\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\\S+\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\\S+\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]\", \" \")\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\", \"\")\n",
    "    df[text_field] = df[text_field].str.lower()\n",
    "    return df\n",
    "\n",
    "# Cleaning noise from train set\n",
    "train = standardize_text(train, 'message')\n",
    "\n",
    "# Cleaning noise from test set\n",
    "test = standardize_text(test, 'message')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# Function to remove all punctuation (,?!.)\n",
    "def remove_punctuation(post):\n",
    "    return ''.join([l for l in post if l not in string.punctuation])\n",
    "\n",
    "# Cleaning punctuation on train set\n",
    "train['message'] = train['message'].apply(remove_punctuation)\n",
    "\n",
    "# Cleaning punctuation on test set\n",
    "test['message'] = test['message'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spliting joint words with word ninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordninja\n",
    "\n",
    "# Function to split join word in train set\n",
    "sentences = []\n",
    "def attached_words(df, message):\n",
    "    for word in message:\n",
    "        word = wordninja.split(word)\n",
    "        sentences.append(\" \".join(word))\n",
    "    df['message'] = sentences\n",
    "    return df\n",
    "\n",
    "# Spliting words on the train set\n",
    "train = attached_words(train,train['message'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split join word in test set\n",
    "\n",
    "sentences1 = []\n",
    "def attached_words(df1, message1):\n",
    "    for word1 in message1:\n",
    "        word1 = wordninja.split(word1)\n",
    "        sentences1.append(\" \".join(word1))\n",
    "    df1['message'] = sentences1\n",
    "    return df1\n",
    "\n",
    "# Spliting words on the test set\n",
    "test = attached_words(test,test['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to replace RT with Retweet\n",
    "def retweets_text(df, text_field):\n",
    "    df[text_field] = df[text_field].str.replace(r\"rt \",\"retweet \")\n",
    "    return df\n",
    "\n",
    "# Replace RT with Retweet on the train set\n",
    "train = retweets_text(train, 'message')\n",
    "\n",
    "# Replace RT with Retweet on the test set\n",
    "test = retweets_text(test, 'message')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords=set(stopwords.words('english'))\n",
    "\n",
    "# words to exclude from the original stopwords set\n",
    "exclude_words = set((\"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", \n",
    "                     'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", \n",
    "                     'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan',\n",
    "                     \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \n",
    "                     'won', \"won't\", 'wouldn', \"wouldn't\", 'not', \"aren't\", \"don't\"))\n",
    "\n",
    "# calc to excluding the above words from original list\n",
    "new_stopwords = stopwords - exclude_words\n",
    "\n",
    "# creating a list from train messages\n",
    "messages_train = train['message'].to_numpy()\n",
    "# creating a list from test messages\n",
    "messages_test = test['message'].to_numpy()\n",
    "\n",
    "# function to remove stopwords from messages\n",
    "def remove_stopwords(messages):\n",
    "    output_array=[]\n",
    "    for sentence in messages:\n",
    "        temp_list=[]\n",
    "        for word in sentence.split():\n",
    "            if word.lower() not in new_stopwords:\n",
    "                temp_list.append(word)\n",
    "        output_array.append(' '.join(temp_list))\n",
    "    return output_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying stopword funtion to train set\n",
    "train['message'] = remove_stopwords(messages_train)\n",
    "# applying stopword funtion to test set\n",
    "test['message'] = remove_stopwords(messages_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>poly sci major epa chief doesnt think carbon d...</td>\n",
       "      <td>625221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>not like lack evidence anthropogenic global wa...</td>\n",
       "      <td>126103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>retweet researchers say three years act climat...</td>\n",
       "      <td>698562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>today maker wired 2016 pivotal year war climat...</td>\n",
       "      <td>573736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>retweet 2016 racist sexist climate change deny...</td>\n",
       "      <td>466954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid\n",
       "0          1  poly sci major epa chief doesnt think carbon d...   625221\n",
       "1          1  not like lack evidence anthropogenic global wa...   126103\n",
       "2          2  retweet researchers say three years act climat...   698562\n",
       "3          1  today maker wired 2016 pivotal year war climat...   573736\n",
       "4          1  retweet 2016 racist sexist climate change deny...   466954"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>europe looking china make sure not alone fight...</td>\n",
       "      <td>169760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>combine polling staffers climate change womens...</td>\n",
       "      <td>35326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>scary unimpeachable evidence climate change al...</td>\n",
       "      <td>224985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>put got jill trump doesnt believe climate chan...</td>\n",
       "      <td>476263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>retweet female orgasms cause global warming sa...</td>\n",
       "      <td>872928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  tweetid\n",
       "0  europe looking china make sure not alone fight...   169760\n",
       "1  combine polling staffers climate change womens...    35326\n",
       "2  scary unimpeachable evidence climate change al...   224985\n",
       "3  put got jill trump doesnt believe climate chan...   476263\n",
       "4  retweet female orgasms cause global warming sa...   872928"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most Frequent Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "climate    13467\n",
       "change     12945\n",
       "retweet     9715\n",
       "global      3884\n",
       "warming     3598\n",
       "trump       2167\n",
       "believe     1161\n",
       "not         1101\n",
       "amp          940\n",
       "doesnt       813\n",
       "dtype: int64"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_freq = pd.Series(' '.join(train['message']).split()).value_counts()[:10]\n",
    "most_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Less Frequent Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "symmetry      1\n",
       "nigh          1\n",
       "accomplice    1\n",
       "salman        1\n",
       "boring        1\n",
       "espanol       1\n",
       "wes           1\n",
       "feb           1\n",
       "ramsey        1\n",
       "blades        1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "less_freq = pd.Series(' '.join(train['message']).split()).value_counts()[-10:]\n",
    "less_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer, word_tokenize\n",
    "tokeniser = TreebankWordTokenizer()\n",
    "\n",
    "#tokenizing the train set\n",
    "train['tokens'] = train['message'].apply(tokeniser.tokenize)\n",
    "#tokenizing the test set\n",
    "test['tokens'] = test['message'].apply(tokeniser.tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "porter_stemmer = PorterStemmer()\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "\n",
    "# function for stemming\n",
    "def dataset_stem(words, stemmer):\n",
    "    return [snowball_stemmer.stem(word) for word in words]\n",
    "\n",
    "# applying stemming to the train set\n",
    "train['snowball_stemmer'] = train['tokens'].apply(dataset_stem, args=(snowball_stemmer,))\n",
    "train['porter_stemmer'] = train['tokens'].apply(dataset_stem, args=(porter_stemmer,))\n",
    "train['lancaster_stemmer'] = train['tokens'].apply(dataset_stem, args=(lancaster_stemmer,))\n",
    "\n",
    "# applying stemming to the test set\n",
    "test['snowball_stemmer'] = test['tokens'].apply(dataset_stem, args=(snowball_stemmer,))\n",
    "test['porter_stemmer'] = test['tokens'].apply(dataset_stem, args=(porter_stemmer,))\n",
    "test['lancaster_stemmer'] = test['tokens'].apply(dataset_stem, args=(lancaster_stemmer,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function for Lemmatizer\n",
    "def dataset_lemma(words, lemmatizer):\n",
    "    return [lemmatizer.lemmatize(word) for word in words]   \n",
    "\n",
    "#Applying lemmatizer of train set\n",
    "train['lemma'] = train['tokens'].apply(dataset_lemma, args=(lemmatizer, ))\n",
    "\n",
    "#Applying lemmatizer of train set\n",
    "test['lemma'] = test['tokens'].apply(dataset_lemma, args=(lemmatizer, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining words in stemming and Lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining words for the snowball stemming on the train set\n",
    "snowball_stem_sentence_train = []\n",
    "for words in train['snowball_stemmer']:\n",
    "    snowball_stem_sentence_train.append(' '.join(words))\n",
    "\n",
    "# joining words for the porter stemming on the train set\n",
    "porter_stem_sentence_train = []\n",
    "for words in train['porter_stemmer']:\n",
    "    porter_stem_sentence_train.append(' '.join(words))\n",
    "\n",
    "# joining words for the lancaster stemming on the train set\n",
    "lancaster_stem_sentence_train = []\n",
    "for words in train['lancaster_stemmer']:\n",
    "    lancaster_stem_sentence_train.append(' '.join(words))\n",
    "\n",
    "# joining words for the lemma on the train set\n",
    "lemma_sentence_train = []\n",
    "for words in train['lemma']:\n",
    "    lemma_sentence_train.append(' '.join(words))\n",
    "\n",
    "# setting lists to the dataframe column\n",
    "train['snowball_stemmer'] = snowball_stem_sentence_train\n",
    "train['porter_stemmer'] = porter_stem_sentence_train\n",
    "train['lancaster_stemmer'] = lancaster_stem_sentence_train\n",
    "train['lemma'] = lemma_sentence_train\n",
    "    \n",
    "\n",
    "# joining words for the snowball stemming on the test set\n",
    "snowball_stem_sentence_test = []\n",
    "for words in test['snowball_stemmer']:\n",
    "    snowball_stem_sentence_test.append(' '.join(words))\n",
    "\n",
    "# joining words for the porter stemming on the test set\n",
    "porter_stem_sentence_test = []\n",
    "for words in test['porter_stemmer']:\n",
    "    porter_stem_sentence_test.append(' '.join(words))\n",
    "\n",
    "# joining words for the lancaster stemming on the test set\n",
    "lancaster_stem_sentence_test = []\n",
    "for words in test['lancaster_stemmer']:\n",
    "    lancaster_stem_sentence_test.append(' '.join(words))\n",
    "\n",
    "# joining words for the lemma on the test set\n",
    "lemma_sentence_test = []\n",
    "for words in test['lemma']:\n",
    "    lemma_sentence_test.append(' '.join(words))\n",
    "\n",
    "# setting lists to the dataframe column\n",
    "test['snowball_stemmer'] = snowball_stem_sentence_test\n",
    "test['porter_stemmer'] = porter_stem_sentence_test\n",
    "test['lancaster_stemmer'] = lancaster_stem_sentence_test\n",
    "test['lemma'] = lemma_sentence_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>tokens</th>\n",
       "      <th>snowball_stemmer</th>\n",
       "      <th>porter_stemmer</th>\n",
       "      <th>lancaster_stemmer</th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>poly sci major epa chief doesnt think carbon d...</td>\n",
       "      <td>625221</td>\n",
       "      <td>[poly, sci, major, epa, chief, doesnt, think, ...</td>\n",
       "      <td>poli sci major epa chief doesnt think carbon d...</td>\n",
       "      <td>poli sci major epa chief doesnt think carbon d...</td>\n",
       "      <td>poli sci major epa chief doesnt think carbon d...</td>\n",
       "      <td>poly sci major epa chief doesnt think carbon d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>not like lack evidence anthropogenic global wa...</td>\n",
       "      <td>126103</td>\n",
       "      <td>[not, like, lack, evidence, anthropogenic, glo...</td>\n",
       "      <td>not like lack evid anthropogen global warm</td>\n",
       "      <td>not like lack evid anthropogen global warm</td>\n",
       "      <td>not like lack evid anthropogen global warm</td>\n",
       "      <td>not like lack evidence anthropogenic global wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>retweet researchers say three years act climat...</td>\n",
       "      <td>698562</td>\n",
       "      <td>[retweet, researchers, say, three, years, act,...</td>\n",
       "      <td>retweet research say three year act climat cha...</td>\n",
       "      <td>retweet research say three year act climat cha...</td>\n",
       "      <td>retweet research say three year act climat cha...</td>\n",
       "      <td>retweet researcher say three year act climate ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>today maker wired 2016 pivotal year war climat...</td>\n",
       "      <td>573736</td>\n",
       "      <td>[today, maker, wired, 2016, pivotal, year, war...</td>\n",
       "      <td>today maker wire 2016 pivot year war climat chang</td>\n",
       "      <td>today maker wire 2016 pivot year war climat chang</td>\n",
       "      <td>today maker wire 2016 pivot year war climat chang</td>\n",
       "      <td>today maker wired 2016 pivotal year war climat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>retweet 2016 racist sexist climate change deny...</td>\n",
       "      <td>466954</td>\n",
       "      <td>[retweet, 2016, racist, sexist, climate, chang...</td>\n",
       "      <td>retweet 2016 racist sexist climat chang deni b...</td>\n",
       "      <td>retweet 2016 racist sexist climat chang deni b...</td>\n",
       "      <td>retweet 2016 racist sexist climat chang deni b...</td>\n",
       "      <td>retweet 2016 racist sexist climate change deny...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid  \\\n",
       "0          1  poly sci major epa chief doesnt think carbon d...   625221   \n",
       "1          1  not like lack evidence anthropogenic global wa...   126103   \n",
       "2          2  retweet researchers say three years act climat...   698562   \n",
       "3          1  today maker wired 2016 pivotal year war climat...   573736   \n",
       "4          1  retweet 2016 racist sexist climate change deny...   466954   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [poly, sci, major, epa, chief, doesnt, think, ...   \n",
       "1  [not, like, lack, evidence, anthropogenic, glo...   \n",
       "2  [retweet, researchers, say, three, years, act,...   \n",
       "3  [today, maker, wired, 2016, pivotal, year, war...   \n",
       "4  [retweet, 2016, racist, sexist, climate, chang...   \n",
       "\n",
       "                                    snowball_stemmer  \\\n",
       "0  poli sci major epa chief doesnt think carbon d...   \n",
       "1         not like lack evid anthropogen global warm   \n",
       "2  retweet research say three year act climat cha...   \n",
       "3  today maker wire 2016 pivot year war climat chang   \n",
       "4  retweet 2016 racist sexist climat chang deni b...   \n",
       "\n",
       "                                      porter_stemmer  \\\n",
       "0  poli sci major epa chief doesnt think carbon d...   \n",
       "1         not like lack evid anthropogen global warm   \n",
       "2  retweet research say three year act climat cha...   \n",
       "3  today maker wire 2016 pivot year war climat chang   \n",
       "4  retweet 2016 racist sexist climat chang deni b...   \n",
       "\n",
       "                                   lancaster_stemmer  \\\n",
       "0  poli sci major epa chief doesnt think carbon d...   \n",
       "1         not like lack evid anthropogen global warm   \n",
       "2  retweet research say three year act climat cha...   \n",
       "3  today maker wire 2016 pivot year war climat chang   \n",
       "4  retweet 2016 racist sexist climat chang deni b...   \n",
       "\n",
       "                                               lemma  \n",
       "0  poly sci major epa chief doesnt think carbon d...  \n",
       "1  not like lack evidence anthropogenic global wa...  \n",
       "2  retweet researcher say three year act climate ...  \n",
       "3  today maker wired 2016 pivotal year war climat...  \n",
       "4  retweet 2016 racist sexist climate change deny...  "
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>tokens</th>\n",
       "      <th>snowball_stemmer</th>\n",
       "      <th>porter_stemmer</th>\n",
       "      <th>lancaster_stemmer</th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>europe looking china make sure not alone fight...</td>\n",
       "      <td>169760</td>\n",
       "      <td>[europe, looking, china, make, sure, not, alon...</td>\n",
       "      <td>europ look china make sure not alon fight clim...</td>\n",
       "      <td>europ look china make sure not alon fight clim...</td>\n",
       "      <td>europ look china make sure not alon fight clim...</td>\n",
       "      <td>europe looking china make sure not alone fight...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>combine polling staffers climate change womens...</td>\n",
       "      <td>35326</td>\n",
       "      <td>[combine, polling, staffers, climate, change, ...</td>\n",
       "      <td>combin poll staffer climat chang women right f...</td>\n",
       "      <td>combin poll staffer climat chang women right f...</td>\n",
       "      <td>combin poll staffer climat chang women right f...</td>\n",
       "      <td>combine polling staffer climate change woman r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>scary unimpeachable evidence climate change al...</td>\n",
       "      <td>224985</td>\n",
       "      <td>[scary, unimpeachable, evidence, climate, chan...</td>\n",
       "      <td>scari unimpeach evid climat chang alreadi time...</td>\n",
       "      <td>scari unimpeach evid climat chang alreadi time...</td>\n",
       "      <td>scari unimpeach evid climat chang alreadi time...</td>\n",
       "      <td>scary unimpeachable evidence climate change al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>put got jill trump doesnt believe climate chan...</td>\n",
       "      <td>476263</td>\n",
       "      <td>[put, got, jill, trump, doesnt, believe, clima...</td>\n",
       "      <td>put got jill trump doesnt believ climat chang ...</td>\n",
       "      <td>put got jill trump doesnt believ climat chang ...</td>\n",
       "      <td>put got jill trump doesnt believ climat chang ...</td>\n",
       "      <td>put got jill trump doesnt believe climate chan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>retweet female orgasms cause global warming sa...</td>\n",
       "      <td>872928</td>\n",
       "      <td>[retweet, female, orgasms, cause, global, warm...</td>\n",
       "      <td>retweet femal orgasm caus global warm sarcast ...</td>\n",
       "      <td>retweet femal orgasm caus global warm sarcast ...</td>\n",
       "      <td>retweet femal orgasm caus global warm sarcast ...</td>\n",
       "      <td>retweet female orgasm cause global warming sar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  tweetid  \\\n",
       "0  europe looking china make sure not alone fight...   169760   \n",
       "1  combine polling staffers climate change womens...    35326   \n",
       "2  scary unimpeachable evidence climate change al...   224985   \n",
       "3  put got jill trump doesnt believe climate chan...   476263   \n",
       "4  retweet female orgasms cause global warming sa...   872928   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [europe, looking, china, make, sure, not, alon...   \n",
       "1  [combine, polling, staffers, climate, change, ...   \n",
       "2  [scary, unimpeachable, evidence, climate, chan...   \n",
       "3  [put, got, jill, trump, doesnt, believe, clima...   \n",
       "4  [retweet, female, orgasms, cause, global, warm...   \n",
       "\n",
       "                                    snowball_stemmer  \\\n",
       "0  europ look china make sure not alon fight clim...   \n",
       "1  combin poll staffer climat chang women right f...   \n",
       "2  scari unimpeach evid climat chang alreadi time...   \n",
       "3  put got jill trump doesnt believ climat chang ...   \n",
       "4  retweet femal orgasm caus global warm sarcast ...   \n",
       "\n",
       "                                      porter_stemmer  \\\n",
       "0  europ look china make sure not alon fight clim...   \n",
       "1  combin poll staffer climat chang women right f...   \n",
       "2  scari unimpeach evid climat chang alreadi time...   \n",
       "3  put got jill trump doesnt believ climat chang ...   \n",
       "4  retweet femal orgasm caus global warm sarcast ...   \n",
       "\n",
       "                                   lancaster_stemmer  \\\n",
       "0  europ look china make sure not alon fight clim...   \n",
       "1  combin poll staffer climat chang women right f...   \n",
       "2  scari unimpeach evid climat chang alreadi time...   \n",
       "3  put got jill trump doesnt believ climat chang ...   \n",
       "4  retweet femal orgasm caus global warm sarcast ...   \n",
       "\n",
       "                                               lemma  \n",
       "0  europe looking china make sure not alone fight...  \n",
       "1  combine polling staffer climate change woman r...  \n",
       "2  scary unimpeachable evidence climate change al...  \n",
       "3  put got jill trump doesnt believe climate chan...  \n",
       "4  retweet female orgasm cause global warming sar...  "
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017369,
     "end_time": "2020-10-07T10:31:44.053776",
     "exception": false,
     "start_time": "2020-10-07T10:31:44.036407",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Splitting out the X variable from the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-07T10:31:44.095546Z",
     "iopub.status.busy": "2020-10-07T10:31:44.094614Z",
     "iopub.status.idle": "2020-10-07T10:31:44.097366Z",
     "shell.execute_reply": "2020-10-07T10:31:44.097898Z"
    },
    "papermill": {
     "duration": 0.026517,
     "end_time": "2020-10-07T10:31:44.098066",
     "exception": false,
     "start_time": "2020-10-07T10:31:44.071549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = train['sentiment']\n",
    "X = train['lemma']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017532,
     "end_time": "2020-10-07T10:31:44.133706",
     "exception": false,
     "start_time": "2020-10-07T10:31:44.116174",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Turning text into something your model can read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-07T10:31:44.204236Z",
     "iopub.status.busy": "2020-10-07T10:31:44.193790Z",
     "iopub.status.idle": "2020-10-07T10:31:45.254842Z",
     "shell.execute_reply": "2020-10-07T10:31:45.254115Z"
    },
    "papermill": {
     "duration": 1.103354,
     "end_time": "2020-10-07T10:31:45.255019",
     "exception": false,
     "start_time": "2020-10-07T10:31:44.151665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df=2)\n",
    "X_vectorized = vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017958,
     "end_time": "2020-10-07T10:31:45.291412",
     "exception": false,
     "start_time": "2020-10-07T10:31:45.273454",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Splitting the training data into a training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-07T10:31:45.334599Z",
     "iopub.status.busy": "2020-10-07T10:31:45.333798Z",
     "iopub.status.idle": "2020-10-07T10:31:45.354945Z",
     "shell.execute_reply": "2020-10-07T10:31:45.354123Z"
    },
    "papermill": {
     "duration": 0.045495,
     "end_time": "2020-10-07T10:31:45.355113",
     "exception": false,
     "start_time": "2020-10-07T10:31:45.309618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train,X_val,y_train,y_val = train_test_split(X_vectorized,y,test_size=0.2,shuffle=True, stratify=y, random_state=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018099,
     "end_time": "2020-10-07T10:31:45.391807",
     "exception": false,
     "start_time": "2020-10-07T10:31:45.373708",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Creating a function to measure best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "def model_selection(model, X_train, X_val, y_train, y_val):\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    return print(f1_score(y_val, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-07T10:31:45.434997Z",
     "iopub.status.busy": "2020-10-07T10:31:45.433825Z",
     "iopub.status.idle": "2020-10-07T10:32:13.157726Z",
     "shell.execute_reply": "2020-10-07T10:32:13.156924Z"
    },
    "papermill": {
     "duration": 27.747703,
     "end_time": "2020-10-07T10:32:13.157852",
     "exception": false,
     "start_time": "2020-10-07T10:31:45.410149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5629160864227646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NL3005\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "model_selection(LogisticRegression(), X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naiive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4474891743978757\n"
     ]
    }
   ],
   "source": [
    "model_selection(MultinomialNB(), X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5312875587452006\n"
     ]
    }
   ],
   "source": [
    "model_selection(RandomForestClassifier(), X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4751660956260282\n"
     ]
    }
   ],
   "source": [
    "model_selection(AdaBoostClassifier(), X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6635216619974396\n"
     ]
    }
   ],
   "source": [
    "rfc = LinearSVC(dual= True,fit_intercept= False,loss= 'hinge',multi_class= 'ovr',penalty= 'l2')\n",
    "model_selection(rfc, X_train, X_val, y_train, y_val)\n",
    "\n",
    "#0.6635216619974396"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters=[{'loss':['hinge'], 'fit_intercept':[False], 'class_weight':['balanced'], 'random_state': [1]}]\n",
    "\n",
    "gscv=GridSearchCV(rfc,parameters,scoring='f1_macro',n_jobs=-1,cv=5)\n",
    "grid_search=gscv.fit(X_vectorized,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_weight': 'balanced',\n",
       " 'fit_intercept': False,\n",
       " 'loss': 'hinge',\n",
       " 'random_state': 1}"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6471141084993387"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_\n",
    "\n",
    "#0.6471141084993387"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5606369527290738\n"
     ]
    }
   ],
   "source": [
    "model_selection(XGBClassifier(), X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNeighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5507078442077513\n"
     ]
    }
   ],
   "source": [
    "model_selection(KNeighborsClassifier(), X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6175817855364216\n"
     ]
    }
   ],
   "source": [
    "model_selection(SVC(kernel = 'linear'), X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5086789983725637\n"
     ]
    }
   ],
   "source": [
    "model_selection(DecisionTreeClassifier(), X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=False,\n",
       "          intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
       "          penalty='l2', random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "rfc = LinearSVC(dual= True,fit_intercept= False,loss= 'hinge',multi_class= 'ovr',penalty= 'l2')\n",
    "rfc.fit(X_vectorized, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>tokens</th>\n",
       "      <th>snowball_stemmer</th>\n",
       "      <th>porter_stemmer</th>\n",
       "      <th>lancaster_stemmer</th>\n",
       "      <th>lemma</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>europe looking china make sure not alone fight...</td>\n",
       "      <td>169760</td>\n",
       "      <td>[europe, looking, china, make, sure, not, alon...</td>\n",
       "      <td>europ look china make sure not alon fight clim...</td>\n",
       "      <td>europ look china make sure not alon fight clim...</td>\n",
       "      <td>europ look china make sure not alon fight clim...</td>\n",
       "      <td>europe looking china make sure not alone fight...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>combine polling staffers climate change womens...</td>\n",
       "      <td>35326</td>\n",
       "      <td>[combine, polling, staffers, climate, change, ...</td>\n",
       "      <td>combin poll staffer climat chang women right f...</td>\n",
       "      <td>combin poll staffer climat chang women right f...</td>\n",
       "      <td>combin poll staffer climat chang women right f...</td>\n",
       "      <td>combine polling staffer climate change woman r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>scary unimpeachable evidence climate change al...</td>\n",
       "      <td>224985</td>\n",
       "      <td>[scary, unimpeachable, evidence, climate, chan...</td>\n",
       "      <td>scari unimpeach evid climat chang alreadi time...</td>\n",
       "      <td>scari unimpeach evid climat chang alreadi time...</td>\n",
       "      <td>scari unimpeach evid climat chang alreadi time...</td>\n",
       "      <td>scary unimpeachable evidence climate change al...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>put got jill trump doesnt believe climate chan...</td>\n",
       "      <td>476263</td>\n",
       "      <td>[put, got, jill, trump, doesnt, believe, clima...</td>\n",
       "      <td>put got jill trump doesnt believ climat chang ...</td>\n",
       "      <td>put got jill trump doesnt believ climat chang ...</td>\n",
       "      <td>put got jill trump doesnt believ climat chang ...</td>\n",
       "      <td>put got jill trump doesnt believe climate chan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>retweet female orgasms cause global warming sa...</td>\n",
       "      <td>872928</td>\n",
       "      <td>[retweet, female, orgasms, cause, global, warm...</td>\n",
       "      <td>retweet femal orgasm caus global warm sarcast ...</td>\n",
       "      <td>retweet femal orgasm caus global warm sarcast ...</td>\n",
       "      <td>retweet femal orgasm caus global warm sarcast ...</td>\n",
       "      <td>retweet female orgasm cause global warming sar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  tweetid  \\\n",
       "0  europe looking china make sure not alone fight...   169760   \n",
       "1  combine polling staffers climate change womens...    35326   \n",
       "2  scary unimpeachable evidence climate change al...   224985   \n",
       "3  put got jill trump doesnt believe climate chan...   476263   \n",
       "4  retweet female orgasms cause global warming sa...   872928   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [europe, looking, china, make, sure, not, alon...   \n",
       "1  [combine, polling, staffers, climate, change, ...   \n",
       "2  [scary, unimpeachable, evidence, climate, chan...   \n",
       "3  [put, got, jill, trump, doesnt, believe, clima...   \n",
       "4  [retweet, female, orgasms, cause, global, warm...   \n",
       "\n",
       "                                    snowball_stemmer  \\\n",
       "0  europ look china make sure not alon fight clim...   \n",
       "1  combin poll staffer climat chang women right f...   \n",
       "2  scari unimpeach evid climat chang alreadi time...   \n",
       "3  put got jill trump doesnt believ climat chang ...   \n",
       "4  retweet femal orgasm caus global warm sarcast ...   \n",
       "\n",
       "                                      porter_stemmer  \\\n",
       "0  europ look china make sure not alon fight clim...   \n",
       "1  combin poll staffer climat chang women right f...   \n",
       "2  scari unimpeach evid climat chang alreadi time...   \n",
       "3  put got jill trump doesnt believ climat chang ...   \n",
       "4  retweet femal orgasm caus global warm sarcast ...   \n",
       "\n",
       "                                   lancaster_stemmer  \\\n",
       "0  europ look china make sure not alon fight clim...   \n",
       "1  combin poll staffer climat chang women right f...   \n",
       "2  scari unimpeach evid climat chang alreadi time...   \n",
       "3  put got jill trump doesnt believ climat chang ...   \n",
       "4  retweet femal orgasm caus global warm sarcast ...   \n",
       "\n",
       "                                               lemma  sentiment  \n",
       "0  europe looking china make sure not alone fight...          1  \n",
       "1  combine polling staffer climate change woman r...          1  \n",
       "2  scary unimpeachable evidence climate change al...          1  \n",
       "3  put got jill trump doesnt believe climate chan...          1  \n",
       "4  retweet female orgasm cause global warming sar...          0  "
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018914,
     "end_time": "2020-10-07T10:32:13.283518",
     "exception": false,
     "start_time": "2020-10-07T10:32:13.264604",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Getting our test set ready "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-07T10:32:13.357774Z",
     "iopub.status.busy": "2020-10-07T10:32:13.347375Z",
     "iopub.status.idle": "2020-10-07T10:32:13.858822Z",
     "shell.execute_reply": "2020-10-07T10:32:13.858192Z"
    },
    "papermill": {
     "duration": 0.556389,
     "end_time": "2020-10-07T10:32:13.858972",
     "exception": false,
     "start_time": "2020-10-07T10:32:13.302583",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "testx = test['lemma']\n",
    "test_vect = vectorizer.transform(testx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01875,
     "end_time": "2020-10-07T10:32:13.896971",
     "exception": false,
     "start_time": "2020-10-07T10:32:13.878221",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Making predictions on the test set and adding a sentiment column to our original test df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-07T10:32:13.944114Z",
     "iopub.status.busy": "2020-10-07T10:32:13.943062Z",
     "iopub.status.idle": "2020-10-07T10:32:14.775936Z",
     "shell.execute_reply": "2020-10-07T10:32:14.775252Z"
    },
    "papermill": {
     "duration": 0.859158,
     "end_time": "2020-10-07T10:32:14.776067",
     "exception": false,
     "start_time": "2020-10-07T10:32:13.916909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = rfc.predict(test_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-07T10:32:14.822256Z",
     "iopub.status.busy": "2020-10-07T10:32:14.821170Z",
     "iopub.status.idle": "2020-10-07T10:32:14.824844Z",
     "shell.execute_reply": "2020-10-07T10:32:14.824131Z"
    },
    "papermill": {
     "duration": 0.029243,
     "end_time": "2020-10-07T10:32:14.824969",
     "exception": false,
     "start_time": "2020-10-07T10:32:14.795726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test['sentiment'] = y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.020114,
     "end_time": "2020-10-07T10:32:14.923436",
     "exception": false,
     "start_time": "2020-10-07T10:32:14.903322",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Creating an output csv for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-07T10:32:14.974377Z",
     "iopub.status.busy": "2020-10-07T10:32:14.970165Z",
     "iopub.status.idle": "2020-10-07T10:32:15.292997Z",
     "shell.execute_reply": "2020-10-07T10:32:15.292333Z"
    },
    "papermill": {
     "duration": 0.349338,
     "end_time": "2020-10-07T10:32:15.293155",
     "exception": false,
     "start_time": "2020-10-07T10:32:14.943817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test[['tweetid','sentiment']].to_csv('testsubmission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 38.75415,
   "end_time": "2020-10-07T10:32:15.462679",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-10-07T10:31:36.708529",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
