{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01786,
     "end_time": "2020-10-07T10:31:41.681660",
     "exception": false,
     "start_time": "2020-10-07T10:31:41.663800",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-10-07T10:31:41.729682Z",
     "iopub.status.busy": "2020-10-07T10:31:41.728858Z",
     "iopub.status.idle": "2020-10-07T10:31:43.774264Z",
     "shell.execute_reply": "2020-10-07T10:31:43.773550Z"
    },
    "papermill": {
     "duration": 2.075554,
     "end_time": "2020-10-07T10:31:43.774414",
     "exception": false,
     "start_time": "2020-10-07T10:31:41.698860",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import re\n",
    "re.compile('<title>(.*)</title>')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016583,
     "end_time": "2020-10-07T10:31:43.808336",
     "exception": false,
     "start_time": "2020-10-07T10:31:43.791753",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-07T10:31:43.857364Z",
     "iopub.status.busy": "2020-10-07T10:31:43.856589Z",
     "iopub.status.idle": "2020-10-07T10:31:43.958708Z",
     "shell.execute_reply": "2020-10-07T10:31:43.958045Z"
    },
    "papermill": {
     "duration": 0.132995,
     "end_time": "2020-10-07T10:31:43.958846",
     "exception": false,
     "start_time": "2020-10-07T10:31:43.825851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-07T10:31:44.013821Z",
     "iopub.status.busy": "2020-10-07T10:31:44.012878Z",
     "iopub.status.idle": "2020-10-07T10:31:44.017979Z",
     "shell.execute_reply": "2020-10-07T10:31:44.017352Z"
    },
    "papermill": {
     "duration": 0.040383,
     "end_time": "2020-10-07T10:31:44.018123",
     "exception": false,
     "start_time": "2020-10-07T10:31:43.977740",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    8530\n",
       " 2    3640\n",
       " 0    2353\n",
       "-1    1296\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
       "      <td>625221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
       "      <td>126103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
       "      <td>698562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
       "      <td>573736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
       "      <td>466954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid\n",
       "0          1  PolySciMajor EPA chief doesn't think carbon di...   625221\n",
       "1          1  It's not like we lack evidence of anthropogeni...   126103\n",
       "2          2  RT @RawStory: Researchers say we have three ye...   698562\n",
       "3          1  #TodayinMaker# WIRED : 2016 was a pivotal year...   573736\n",
       "4          1  RT @SoyNovioDeTodas: It's 2016, and a racist, ...   466954"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_text(df, text_field):\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\\S+\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\\S+\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]\", \" \")\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"RT \",\"Retweet \")\n",
    "    df[text_field] = df[text_field].str.lower()\n",
    "    return df\n",
    "\n",
    "train = standardize_text(train, 'message')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PolySciMajor EPA chief doesnt think carbon dio...</td>\n",
       "      <td>625221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Its not like we lack evidence of anthropogenic...</td>\n",
       "      <td>126103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Retweet  Researchers say we have three years t...</td>\n",
       "      <td>698562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>TodayinMaker  WIRED   2016 was a pivotal year...</td>\n",
       "      <td>573736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Retweet  Its 2016 and a racist sexist climate ...</td>\n",
       "      <td>466954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid\n",
       "0          1  PolySciMajor EPA chief doesnt think carbon dio...   625221\n",
       "1          1  Its not like we lack evidence of anthropogenic...   126103\n",
       "2          2  Retweet  Researchers say we have three years t...   698562\n",
       "3          1   TodayinMaker  WIRED   2016 was a pivotal year...   573736\n",
       "4          1  Retweet  Its 2016 and a racist sexist climate ...   466954"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def remove_punctuation(post):\n",
    "    return ''.join([l for l in post if l not in string.punctuation])\n",
    "\n",
    "train['message'] = train['message'].apply(remove_punctuation)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords=set(stopwords.words('english'))\n",
    "exclude_words = set((\"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", \n",
    "                     'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", \n",
    "                     'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan',\n",
    "                     \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \n",
    "                     'won', \"won't\", 'wouldn', \"wouldn't\", 'not', \"aren't\", \"don't\"))\n",
    "\n",
    "new_stopwords = stopwords - exclude_words\n",
    "\n",
    "messages = train['message'].to_numpy()\n",
    "\n",
    "def remove_stopwords(messages):\n",
    "    output_array=[]\n",
    "    for sentence in messages:\n",
    "        temp_list=[]\n",
    "        for word in sentence.split():\n",
    "            if word.lower() not in new_stopwords:\n",
    "                temp_list.append(word)\n",
    "        output_array.append(' '.join(temp_list))\n",
    "    return output_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PolySciMajor EPA chief doesnt think carbon dio...</td>\n",
       "      <td>625221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>not like lack evidence anthropogenic global wa...</td>\n",
       "      <td>126103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Retweet Researchers say three years act climat...</td>\n",
       "      <td>698562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>TodayinMaker WIRED 2016 pivotal year war clima...</td>\n",
       "      <td>573736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Retweet 2016 racist sexist climate change deny...</td>\n",
       "      <td>466954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>Worth read whether dont believe climate change</td>\n",
       "      <td>425577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>Retweet Mike Pence doesn believe global warmin...</td>\n",
       "      <td>294933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>Retweet Six big things today fight climate cha...</td>\n",
       "      <td>992717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>8yo nephew inconsolable wants die old age like...</td>\n",
       "      <td>664510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>Retweet offense like not believe global warming</td>\n",
       "      <td>260471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid\n",
       "0          1  PolySciMajor EPA chief doesnt think carbon dio...   625221\n",
       "1          1  not like lack evidence anthropogenic global wa...   126103\n",
       "2          2  Retweet Researchers say three years act climat...   698562\n",
       "3          1  TodayinMaker WIRED 2016 pivotal year war clima...   573736\n",
       "4          1  Retweet 2016 racist sexist climate change deny...   466954\n",
       "5          1     Worth read whether dont believe climate change   425577\n",
       "6          1  Retweet Mike Pence doesn believe global warmin...   294933\n",
       "7          1  Retweet Six big things today fight climate cha...   992717\n",
       "8          1  8yo nephew inconsolable wants die old age like...   664510\n",
       "9          1    Retweet offense like not believe global warming   260471"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['message'] = remove_stopwords(messages)\n",
    "train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most Frequent Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "climate    12838\n",
       "change     12660\n",
       "Retweet     9724\n",
       "global      3707\n",
       "warming     3516\n",
       "Trump       1985\n",
       "believe     1158\n",
       "not          977\n",
       "amp          940\n",
       "doesnt       802\n",
       "dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_freq = pd.Series(' '.join(train['message']).split()).value_counts()[:10]\n",
    "most_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Less Frequent Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hank                 1\n",
       "Dies                 1\n",
       "treading             1\n",
       "Room                 1\n",
       "kiss                 1\n",
       "Fishel               1\n",
       "science2016electi    1\n",
       "inve                 1\n",
       "pts                  1\n",
       "sinc                 1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "less_freq = pd.Series(' '.join(train['message']).split()).value_counts()[-10:]\n",
    "less_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PolySciMajor EPA chief doesnt think carbon dio...</td>\n",
       "      <td>625221</td>\n",
       "      <td>[PolySciMajor, EPA, chief, doesnt, think, carb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>not like lack evidence anthropogenic global wa...</td>\n",
       "      <td>126103</td>\n",
       "      <td>[not, like, lack, evidence, anthropogenic, glo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Retweet Researchers say three years act climat...</td>\n",
       "      <td>698562</td>\n",
       "      <td>[Retweet, Researchers, say, three, years, act,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>TodayinMaker WIRED 2016 pivotal year war clima...</td>\n",
       "      <td>573736</td>\n",
       "      <td>[TodayinMaker, WIRED, 2016, pivotal, year, war...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Retweet 2016 racist sexist climate change deny...</td>\n",
       "      <td>466954</td>\n",
       "      <td>[Retweet, 2016, racist, sexist, climate, chang...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid  \\\n",
       "0          1  PolySciMajor EPA chief doesnt think carbon dio...   625221   \n",
       "1          1  not like lack evidence anthropogenic global wa...   126103   \n",
       "2          2  Retweet Researchers say three years act climat...   698562   \n",
       "3          1  TodayinMaker WIRED 2016 pivotal year war clima...   573736   \n",
       "4          1  Retweet 2016 racist sexist climate change deny...   466954   \n",
       "\n",
       "                                              tokens  \n",
       "0  [PolySciMajor, EPA, chief, doesnt, think, carb...  \n",
       "1  [not, like, lack, evidence, anthropogenic, glo...  \n",
       "2  [Retweet, Researchers, say, three, years, act,...  \n",
       "3  [TodayinMaker, WIRED, 2016, pivotal, year, war...  \n",
       "4  [Retweet, 2016, racist, sexist, climate, chang...  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer, word_tokenize\n",
    "\n",
    "tokeniser = TreebankWordTokenizer()\n",
    "train['tokens'] = train['message'].apply(tokeniser.tokenize)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>tokens</th>\n",
       "      <th>stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>polyscimajor epa chief doesnt think carbon dio...</td>\n",
       "      <td>625221</td>\n",
       "      <td>[polyscimajor, epa, chief, doesnt, think, carb...</td>\n",
       "      <td>[polyscimajor, epa, chief, doesnt, think, carb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>not like lack evidence anthropogenic global wa...</td>\n",
       "      <td>126103</td>\n",
       "      <td>[not, like, lack, evidence, anthropogenic, glo...</td>\n",
       "      <td>[not, like, lack, evid, anthropogen, global, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>retweet researchers say three years act climat...</td>\n",
       "      <td>698562</td>\n",
       "      <td>[retweet, researchers, say, three, years, act,...</td>\n",
       "      <td>[retweet, research, say, three, year, act, cli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>todayinmaker wired 2016 pivotal year war clima...</td>\n",
       "      <td>573736</td>\n",
       "      <td>[todayinmaker, wired, 2016, pivotal, year, war...</td>\n",
       "      <td>[todayinmak, wire, 2016, pivot, year, war, cli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>retweet 2016 racist sexist climate change deny...</td>\n",
       "      <td>466954</td>\n",
       "      <td>[retweet, 2016, racist, sexist, climate, chang...</td>\n",
       "      <td>[retweet, 2016, racist, sexist, climat, chang,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid  \\\n",
       "0          1  polyscimajor epa chief doesnt think carbon dio...   625221   \n",
       "1          1  not like lack evidence anthropogenic global wa...   126103   \n",
       "2          2  retweet researchers say three years act climat...   698562   \n",
       "3          1  todayinmaker wired 2016 pivotal year war clima...   573736   \n",
       "4          1  retweet 2016 racist sexist climate change deny...   466954   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [polyscimajor, epa, chief, doesnt, think, carb...   \n",
       "1  [not, like, lack, evidence, anthropogenic, glo...   \n",
       "2  [retweet, researchers, say, three, years, act,...   \n",
       "3  [todayinmaker, wired, 2016, pivotal, year, war...   \n",
       "4  [retweet, 2016, racist, sexist, climate, chang...   \n",
       "\n",
       "                                                stem  \n",
       "0  [polyscimajor, epa, chief, doesnt, think, carb...  \n",
       "1  [not, like, lack, evid, anthropogen, global, w...  \n",
       "2  [retweet, research, say, three, year, act, cli...  \n",
       "3  [todayinmak, wire, 2016, pivot, year, war, cli...  \n",
       "4  [retweet, 2016, racist, sexist, climat, chang,...  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def dataset_stem(words, stemmer):\n",
    "    return [stemmer.stem(word) for word in words]\n",
    "\n",
    "train['stem'] = train['tokens'].apply(dataset_stem, args=(stemmer,))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>tokens</th>\n",
       "      <th>stem</th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>polyscimajor epa chief doesnt think carbon dio...</td>\n",
       "      <td>625221</td>\n",
       "      <td>[polyscimajor, epa, chief, doesnt, think, carb...</td>\n",
       "      <td>[polyscimajor, epa, chief, doesnt, think, carb...</td>\n",
       "      <td>[polyscimajor, epa, chief, doesnt, think, carb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>not like lack evidence anthropogenic global wa...</td>\n",
       "      <td>126103</td>\n",
       "      <td>[not, like, lack, evidence, anthropogenic, glo...</td>\n",
       "      <td>[not, like, lack, evid, anthropogen, global, w...</td>\n",
       "      <td>[not, like, lack, evidence, anthropogenic, glo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>retweet researchers say three years act climat...</td>\n",
       "      <td>698562</td>\n",
       "      <td>[retweet, researchers, say, three, years, act,...</td>\n",
       "      <td>[retweet, research, say, three, year, act, cli...</td>\n",
       "      <td>[retweet, researcher, say, three, year, act, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>todayinmaker wired 2016 pivotal year war clima...</td>\n",
       "      <td>573736</td>\n",
       "      <td>[todayinmaker, wired, 2016, pivotal, year, war...</td>\n",
       "      <td>[todayinmak, wire, 2016, pivot, year, war, cli...</td>\n",
       "      <td>[todayinmaker, wired, 2016, pivotal, year, war...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>retweet 2016 racist sexist climate change deny...</td>\n",
       "      <td>466954</td>\n",
       "      <td>[retweet, 2016, racist, sexist, climate, chang...</td>\n",
       "      <td>[retweet, 2016, racist, sexist, climat, chang,...</td>\n",
       "      <td>[retweet, 2016, racist, sexist, climate, chang...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid  \\\n",
       "0          1  polyscimajor epa chief doesnt think carbon dio...   625221   \n",
       "1          1  not like lack evidence anthropogenic global wa...   126103   \n",
       "2          2  retweet researchers say three years act climat...   698562   \n",
       "3          1  todayinmaker wired 2016 pivotal year war clima...   573736   \n",
       "4          1  retweet 2016 racist sexist climate change deny...   466954   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [polyscimajor, epa, chief, doesnt, think, carb...   \n",
       "1  [not, like, lack, evidence, anthropogenic, glo...   \n",
       "2  [retweet, researchers, say, three, years, act,...   \n",
       "3  [todayinmaker, wired, 2016, pivotal, year, war...   \n",
       "4  [retweet, 2016, racist, sexist, climate, chang...   \n",
       "\n",
       "                                                stem  \\\n",
       "0  [polyscimajor, epa, chief, doesnt, think, carb...   \n",
       "1  [not, like, lack, evid, anthropogen, global, w...   \n",
       "2  [retweet, research, say, three, year, act, cli...   \n",
       "3  [todayinmak, wire, 2016, pivot, year, war, cli...   \n",
       "4  [retweet, 2016, racist, sexist, climat, chang,...   \n",
       "\n",
       "                                               lemma  \n",
       "0  [polyscimajor, epa, chief, doesnt, think, carb...  \n",
       "1  [not, like, lack, evidence, anthropogenic, glo...  \n",
       "2  [retweet, researcher, say, three, year, act, c...  \n",
       "3  [todayinmaker, wired, 2016, pivotal, year, war...  \n",
       "4  [retweet, 2016, racist, sexist, climate, chang...  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def dataset_lemma(words, lemmatizer):\n",
    "    return [lemmatizer.lemmatize(word) for word in words]   \n",
    "\n",
    "train['lemma'] = train['tokens'].apply(dataset_lemma, args=(lemmatizer, ))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining words in stemming and Lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>tokens</th>\n",
       "      <th>stem</th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>polyscimajor epa chief doesnt think carbon dio...</td>\n",
       "      <td>625221</td>\n",
       "      <td>[polyscimajor, epa, chief, doesnt, think, carb...</td>\n",
       "      <td>polyscimajor epa chief doesnt think carbon dio...</td>\n",
       "      <td>polyscimajor epa chief doesnt think carbon dio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>not like lack evidence anthropogenic global wa...</td>\n",
       "      <td>126103</td>\n",
       "      <td>[not, like, lack, evidence, anthropogenic, glo...</td>\n",
       "      <td>not like lack evid anthropogen global warm</td>\n",
       "      <td>not like lack evidence anthropogenic global wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>retweet researchers say three years act climat...</td>\n",
       "      <td>698562</td>\n",
       "      <td>[retweet, researchers, say, three, years, act,...</td>\n",
       "      <td>retweet research say three year act climat cha...</td>\n",
       "      <td>retweet researcher say three year act climate ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>todayinmaker wired 2016 pivotal year war clima...</td>\n",
       "      <td>573736</td>\n",
       "      <td>[todayinmaker, wired, 2016, pivotal, year, war...</td>\n",
       "      <td>todayinmak wire 2016 pivot year war climat chang</td>\n",
       "      <td>todayinmaker wired 2016 pivotal year war clima...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>retweet 2016 racist sexist climate change deny...</td>\n",
       "      <td>466954</td>\n",
       "      <td>[retweet, 2016, racist, sexist, climate, chang...</td>\n",
       "      <td>retweet 2016 racist sexist climat chang deni b...</td>\n",
       "      <td>retweet 2016 racist sexist climate change deny...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid  \\\n",
       "0          1  polyscimajor epa chief doesnt think carbon dio...   625221   \n",
       "1          1  not like lack evidence anthropogenic global wa...   126103   \n",
       "2          2  retweet researchers say three years act climat...   698562   \n",
       "3          1  todayinmaker wired 2016 pivotal year war clima...   573736   \n",
       "4          1  retweet 2016 racist sexist climate change deny...   466954   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [polyscimajor, epa, chief, doesnt, think, carb...   \n",
       "1  [not, like, lack, evidence, anthropogenic, glo...   \n",
       "2  [retweet, researchers, say, three, years, act,...   \n",
       "3  [todayinmaker, wired, 2016, pivotal, year, war...   \n",
       "4  [retweet, 2016, racist, sexist, climate, chang...   \n",
       "\n",
       "                                                stem  \\\n",
       "0  polyscimajor epa chief doesnt think carbon dio...   \n",
       "1         not like lack evid anthropogen global warm   \n",
       "2  retweet research say three year act climat cha...   \n",
       "3   todayinmak wire 2016 pivot year war climat chang   \n",
       "4  retweet 2016 racist sexist climat chang deni b...   \n",
       "\n",
       "                                               lemma  \n",
       "0  polyscimajor epa chief doesnt think carbon dio...  \n",
       "1  not like lack evidence anthropogenic global wa...  \n",
       "2  retweet researcher say three year act climate ...  \n",
       "3  todayinmaker wired 2016 pivotal year war clima...  \n",
       "4  retweet 2016 racist sexist climate change deny...  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_sentence = []\n",
    "for words in train['lemma']:\n",
    "    lemma_sentence.append(' '.join(words))\n",
    "\n",
    "stem_sentence = []\n",
    "for words in train['stem']:\n",
    "    stem_sentence.append(' '.join(words))\n",
    "    \n",
    "train['lemma'] = lemma_sentence\n",
    "train['stem'] = stem_sentence\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017369,
     "end_time": "2020-10-07T10:31:44.053776",
     "exception": false,
     "start_time": "2020-10-07T10:31:44.036407",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Splitting out the X variable from the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-07T10:31:44.095546Z",
     "iopub.status.busy": "2020-10-07T10:31:44.094614Z",
     "iopub.status.idle": "2020-10-07T10:31:44.097366Z",
     "shell.execute_reply": "2020-10-07T10:31:44.097898Z"
    },
    "papermill": {
     "duration": 0.026517,
     "end_time": "2020-10-07T10:31:44.098066",
     "exception": false,
     "start_time": "2020-10-07T10:31:44.071549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = train['sentiment']\n",
    "X = train['stem']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017532,
     "end_time": "2020-10-07T10:31:44.133706",
     "exception": false,
     "start_time": "2020-10-07T10:31:44.116174",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Turning text into something your model can read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-07T10:31:44.204236Z",
     "iopub.status.busy": "2020-10-07T10:31:44.193790Z",
     "iopub.status.idle": "2020-10-07T10:31:45.254842Z",
     "shell.execute_reply": "2020-10-07T10:31:45.254115Z"
    },
    "papermill": {
     "duration": 1.103354,
     "end_time": "2020-10-07T10:31:45.255019",
     "exception": false,
     "start_time": "2020-10-07T10:31:44.151665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df=2)\n",
    "X_vectorized = vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017958,
     "end_time": "2020-10-07T10:31:45.291412",
     "exception": false,
     "start_time": "2020-10-07T10:31:45.273454",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Splitting the training data into a training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-07T10:31:45.334599Z",
     "iopub.status.busy": "2020-10-07T10:31:45.333798Z",
     "iopub.status.idle": "2020-10-07T10:31:45.354945Z",
     "shell.execute_reply": "2020-10-07T10:31:45.354123Z"
    },
    "papermill": {
     "duration": 0.045495,
     "end_time": "2020-10-07T10:31:45.355113",
     "exception": false,
     "start_time": "2020-10-07T10:31:45.309618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train,X_val,y_train,y_val = train_test_split(X_vectorized,y,test_size=0.2,shuffle=True, stratify=y, random_state=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018099,
     "end_time": "2020-10-07T10:31:45.391807",
     "exception": false,
     "start_time": "2020-10-07T10:31:45.373708",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Creating a function to measure best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "def model_selection(model, X_train, X_val, y_train, y_val):\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    return print(f1_score(y_val, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-07T10:31:45.434997Z",
     "iopub.status.busy": "2020-10-07T10:31:45.433825Z",
     "iopub.status.idle": "2020-10-07T10:32:13.157726Z",
     "shell.execute_reply": "2020-10-07T10:32:13.156924Z"
    },
    "papermill": {
     "duration": 27.747703,
     "end_time": "2020-10-07T10:32:13.157852",
     "exception": false,
     "start_time": "2020-10-07T10:31:45.410149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5876026123084404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NL3005\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "model_selection(LogisticRegression(), X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naiive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45924109081425507\n"
     ]
    }
   ],
   "source": [
    "model_selection(MultinomialNB(), X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5576799864496926\n"
     ]
    }
   ],
   "source": [
    "model_selection(RandomForestClassifier(), X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47913283189835626\n"
     ]
    }
   ],
   "source": [
    "model_selection(AdaBoostClassifier(), X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6285113650693761\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': 0.3, 'fit_intercept': False, 'random_state': 1}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = LinearSVC(C=0.3, fit_intercept=False, random_state=1)\n",
    "model_selection(rfc, X_train, X_val, y_train, y_val)\n",
    "\n",
    "{'C': 0.3, 'fit_intercept': False, 'random_state': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVC with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "rfc = LinearSVC()\n",
    "parameters=[{'C':[0.1,0.2,0.3],'fit_intercept':[True, False],'random_state':[1,2,3,4,5,6]}]\n",
    "\n",
    "gscv = GridSearchCV(rfc,parameters,scoring='f1_macro',n_jobs=-1,cv=5)\n",
    "grid_search=gscv.fit(X_vectorized,y)\n",
    "\n",
    "#penalty='l2',\n",
    "#loss='squared_hinge',\n",
    "#dual=True,\n",
    "#tol=0.0001,\n",
    "#C=1.0,\n",
    "#multi_class='ovr',\n",
    "#fit_intercept=True,\n",
    "#intercept_scaling=1,\n",
    "#class_weight=None,\n",
    "#verbose=0,\n",
    "#random_state=None,\n",
    "#max_iter=1000,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.3, 'fit_intercept': False, 'random_state': 1}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5641932390457639\n"
     ]
    }
   ],
   "source": [
    "model_selection(XGBClassifier(), X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNeighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5291337551408124\n"
     ]
    }
   ],
   "source": [
    "model_selection(KNeighborsClassifier(), X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5754640429191721\n"
     ]
    }
   ],
   "source": [
    "model_selection(SVC(), X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.514376125007167\n"
     ]
    }
   ],
   "source": [
    "model_selection(DecisionTreeClassifier(), X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=0.3, class_weight=None, dual=True, fit_intercept=False,\n",
       "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "          multi_class='ovr', penalty='l2', random_state=1, tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "rfc = LinearSVC(C=0.3, fit_intercept=False, random_state=1)\n",
    "rfc.fit(X_vectorized, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = standardize_text(test, 'message')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['message'] = test['message'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['tokens'] = test['message'].apply(tokeniser.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['stem'] = test['tokens'].apply(dataset_stem, args=(stemmer,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['lemma'] = test['tokens'].apply(dataset_lemma, args=(lemmatizer, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>tokens</th>\n",
       "      <th>stem</th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>europe will now be looking to china to make su...</td>\n",
       "      <td>169760</td>\n",
       "      <td>[europe, will, now, be, looking, to, china, to...</td>\n",
       "      <td>europ will now be look to china to make sure t...</td>\n",
       "      <td>europe will now be looking to china to make su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>combine this with the polling of staffers re c...</td>\n",
       "      <td>35326</td>\n",
       "      <td>[combine, this, with, the, polling, of, staffe...</td>\n",
       "      <td>combin this with the poll of staffer re climat...</td>\n",
       "      <td>combine this with the polling of staffer re cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the scary unimpeachable evidence that climate ...</td>\n",
       "      <td>224985</td>\n",
       "      <td>[the, scary, unimpeachable, evidence, that, cl...</td>\n",
       "      <td>the scari unimpeach evid that climat chang is ...</td>\n",
       "      <td>the scary unimpeachable evidence that climate ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nputin got to you too jill  \\ntrump doesn...</td>\n",
       "      <td>476263</td>\n",
       "      <td>[putin, got, to, you, too, jill, trump, doesnt...</td>\n",
       "      <td>putin got to you too jill trump doesnt believ ...</td>\n",
       "      <td>putin got to you too jill trump doesnt believe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>retweet  female orgasms cause global warming\\n...</td>\n",
       "      <td>872928</td>\n",
       "      <td>[retweet, female, orgasms, cause, global, warm...</td>\n",
       "      <td>retweet femal orgasm caus global warm sarcast ...</td>\n",
       "      <td>retweet female orgasm cause global warming sar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  tweetid  \\\n",
       "0  europe will now be looking to china to make su...   169760   \n",
       "1  combine this with the polling of staffers re c...    35326   \n",
       "2  the scary unimpeachable evidence that climate ...   224985   \n",
       "3      \\nputin got to you too jill  \\ntrump doesn...   476263   \n",
       "4  retweet  female orgasms cause global warming\\n...   872928   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [europe, will, now, be, looking, to, china, to...   \n",
       "1  [combine, this, with, the, polling, of, staffe...   \n",
       "2  [the, scary, unimpeachable, evidence, that, cl...   \n",
       "3  [putin, got, to, you, too, jill, trump, doesnt...   \n",
       "4  [retweet, female, orgasms, cause, global, warm...   \n",
       "\n",
       "                                                stem  \\\n",
       "0  europ will now be look to china to make sure t...   \n",
       "1  combin this with the poll of staffer re climat...   \n",
       "2  the scari unimpeach evid that climat chang is ...   \n",
       "3  putin got to you too jill trump doesnt believ ...   \n",
       "4  retweet femal orgasm caus global warm sarcast ...   \n",
       "\n",
       "                                               lemma  \n",
       "0  europe will now be looking to china to make su...  \n",
       "1  combine this with the polling of staffer re cl...  \n",
       "2  the scary unimpeachable evidence that climate ...  \n",
       "3  putin got to you too jill trump doesnt believe...  \n",
       "4  retweet female orgasm cause global warming sar...  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_sentence_1 = []\n",
    "for words in test['lemma']:\n",
    "    lemma_sentence_1.append(' '.join(words))\n",
    "\n",
    "stem_sentence_1 = []\n",
    "for words in test['stem']:\n",
    "    stem_sentence_1.append(' '.join(words))\n",
    "    \n",
    "test['lemma'] = lemma_sentence_1\n",
    "test['stem'] = stem_sentence_1\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018914,
     "end_time": "2020-10-07T10:32:13.283518",
     "exception": false,
     "start_time": "2020-10-07T10:32:13.264604",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Getting our test set ready "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-07T10:32:13.357774Z",
     "iopub.status.busy": "2020-10-07T10:32:13.347375Z",
     "iopub.status.idle": "2020-10-07T10:32:13.858822Z",
     "shell.execute_reply": "2020-10-07T10:32:13.858192Z"
    },
    "papermill": {
     "duration": 0.556389,
     "end_time": "2020-10-07T10:32:13.858972",
     "exception": false,
     "start_time": "2020-10-07T10:32:13.302583",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "testx = test['stem']\n",
    "test_vect = vectorizer.transform(testx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01875,
     "end_time": "2020-10-07T10:32:13.896971",
     "exception": false,
     "start_time": "2020-10-07T10:32:13.878221",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Making predictions on the test set and adding a sentiment column to our original test df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-07T10:32:13.944114Z",
     "iopub.status.busy": "2020-10-07T10:32:13.943062Z",
     "iopub.status.idle": "2020-10-07T10:32:14.775936Z",
     "shell.execute_reply": "2020-10-07T10:32:14.775252Z"
    },
    "papermill": {
     "duration": 0.859158,
     "end_time": "2020-10-07T10:32:14.776067",
     "exception": false,
     "start_time": "2020-10-07T10:32:13.916909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = rfc.predict(test_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-07T10:32:14.822256Z",
     "iopub.status.busy": "2020-10-07T10:32:14.821170Z",
     "iopub.status.idle": "2020-10-07T10:32:14.824844Z",
     "shell.execute_reply": "2020-10-07T10:32:14.824131Z"
    },
    "papermill": {
     "duration": 0.029243,
     "end_time": "2020-10-07T10:32:14.824969",
     "exception": false,
     "start_time": "2020-10-07T10:32:14.795726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test['sentiment'] = y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.020114,
     "end_time": "2020-10-07T10:32:14.923436",
     "exception": false,
     "start_time": "2020-10-07T10:32:14.903322",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Creating an output csv for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-07T10:32:14.974377Z",
     "iopub.status.busy": "2020-10-07T10:32:14.970165Z",
     "iopub.status.idle": "2020-10-07T10:32:15.292997Z",
     "shell.execute_reply": "2020-10-07T10:32:15.292333Z"
    },
    "papermill": {
     "duration": 0.349338,
     "end_time": "2020-10-07T10:32:15.293155",
     "exception": false,
     "start_time": "2020-10-07T10:32:14.943817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test[['tweetid','sentiment']].to_csv('testsubmission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 38.75415,
   "end_time": "2020-10-07T10:32:15.462679",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-10-07T10:31:36.708529",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
