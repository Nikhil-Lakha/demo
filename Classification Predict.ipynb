{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01786,
     "end_time": "2020-10-07T10:31:41.681660",
     "exception": false,
     "start_time": "2020-10-07T10:31:41.663800",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-10-07T10:31:41.729682Z",
     "iopub.status.busy": "2020-10-07T10:31:41.728858Z",
     "iopub.status.idle": "2020-10-07T10:31:43.774264Z",
     "shell.execute_reply": "2020-10-07T10:31:43.773550Z"
    },
    "papermill": {
     "duration": 2.075554,
     "end_time": "2020-10-07T10:31:43.774414",
     "exception": false,
     "start_time": "2020-10-07T10:31:41.698860",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import re\n",
    "re.compile('<title>(.*)</title>')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016583,
     "end_time": "2020-10-07T10:31:43.808336",
     "exception": false,
     "start_time": "2020-10-07T10:31:43.791753",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-07T10:31:43.857364Z",
     "iopub.status.busy": "2020-10-07T10:31:43.856589Z",
     "iopub.status.idle": "2020-10-07T10:31:43.958708Z",
     "shell.execute_reply": "2020-10-07T10:31:43.958045Z"
    },
    "papermill": {
     "duration": 0.132995,
     "end_time": "2020-10-07T10:31:43.958846",
     "exception": false,
     "start_time": "2020-10-07T10:31:43.825851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-07T10:31:44.013821Z",
     "iopub.status.busy": "2020-10-07T10:31:44.012878Z",
     "iopub.status.idle": "2020-10-07T10:31:44.017979Z",
     "shell.execute_reply": "2020-10-07T10:31:44.017352Z"
    },
    "papermill": {
     "duration": 0.040383,
     "end_time": "2020-10-07T10:31:44.018123",
     "exception": false,
     "start_time": "2020-10-07T10:31:43.977740",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    8530\n",
       " 2    3640\n",
       " 0    2353\n",
       "-1    1296\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to clean out any noise from the data\n",
    "def standardize_text(df, text_field):\n",
    "    \n",
    "    df[text_field] = df[text_field].str.replace(r\"http\\S+\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r'&amp;?',r'and')\n",
    "    df[text_field] = df[text_field].str.replace(r'&lt;',r'<')\n",
    "    df[text_field] = df[text_field].str.replace(r'&gt;',r'>')\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\\S+\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]\", \" \")\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\", \"\")\n",
    "    df[text_field] = df[text_field].str.lower()\n",
    "    return df\n",
    "\n",
    "# Cleaning noise from train set\n",
    "train = standardize_text(train, 'message')\n",
    "\n",
    "# Cleaning noise from test set\n",
    "test = standardize_text(test, 'message')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace apostrophe/short words in python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# Function to remove all punctuation (,?!.)\n",
    "def remove_punctuation(post):\n",
    "    return ''.join([l for l in post if l not in string.punctuation])\n",
    "\n",
    "# Cleaning punctuation on train set\n",
    "train['new message'] = train['message'].apply(remove_punctuation)\n",
    "\n",
    "# Cleaning punctuation on test set\n",
    "test['new message'] = test['message'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spliting joint words with word ninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordninja\n",
    "\n",
    "# Function to split join word in train set\n",
    "sentences = []\n",
    "def attached_words(df, message):\n",
    "    for word in message:\n",
    "        word = wordninja.split(word)\n",
    "        sentences.append(\" \".join(word))\n",
    "    df['new message'] = sentences\n",
    "    return df\n",
    "\n",
    "# Spliting words on the train set\n",
    "train = attached_words(train,train['new message'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split join word in test set\n",
    "\n",
    "sentences1 = []\n",
    "def attached_words(df1, message1):\n",
    "    for word1 in message1:\n",
    "        word1 = wordninja.split(word1)\n",
    "        sentences1.append(\" \".join(word1))\n",
    "    df1['new message'] = sentences1\n",
    "    return df1\n",
    "\n",
    "# Spliting words on the test set\n",
    "test = attached_words(test,test['new message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to replace RT with Retweet\n",
    "def retweets_text(df, text_field):\n",
    "    df[text_field] = df[text_field].str.replace(r\"rt \",\"retweet \")\n",
    "    return df\n",
    "\n",
    "# Replace RT with Retweet on the train set\n",
    "train = retweets_text(train, 'new message')\n",
    "\n",
    "# Replace RT with Retweet on the test set\n",
    "test = retweets_text(test, 'new message')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "custom_stopwords = ['to', 'through', 'if', 'yours', 'such', 'your', 'i', 'because', 'at', 'now', 'her', 'it', \n",
    "                    'weren', 'don', 'were', 'all', 'above', 'once', 'any', 'as', \"you'd\", 'but', 'did', \n",
    "                    'the', \"it's\", 'them', \"should've\", 'down', 'they', 'in', 'below', 'she', 'up', \n",
    "                    'my', 'doing', 'themselves', 'which', 'you', 'into', 'each', 'very', \n",
    "                    'ourselves', 'yourselves', 'then', 'ain', 'should', 'most', \"you're\", \n",
    "                    'this', 'there', 'yourself', 'where', 'with', 'about', \n",
    "                    'from', 'so', 'do', \"that'll\", 'same', 'some', 'what', 'too', 'further', \n",
    "                    'before', 'having', 'their', 'out', 'those', 'of', 'few', 'than', \n",
    "                    'our', 'is', 'against', 'off', 'by', 'can', 'are', 'will', 'and', \n",
    "                    'when', 'hers', \"you've\", 'being', 'for', 'just', 'between', 'here', 'have', \n",
    "                    'myself', 'been', 'am', 'own', 'no', 'needn', 'd', 'whom', 'during', \n",
    "                    'only', 'or', 'after', 'until', 'be', 'has', 'on', 'other', \"you'll\", 'we', 'more', \n",
    "                    'a', \"she's\", 'again', 've', 'he', 'himself', 'was', 'over', 'me', 'its', \n",
    "                    'itself', 'theirs', 'these', 'had', 'who', 'while', 'that', 'how', 'does', 'ours', 'y', \n",
    "                    're', 'herself', 'both', 'an', 'his', 'him', 'under', 'why']\n",
    "\n",
    "\n",
    "\n",
    "# words to exclude from the original stopwords set\n",
    "\n",
    "\n",
    "# calc to excluding the above words from original list\n",
    "#new_stopwords = stopwords - exclude_words\n",
    "\n",
    "# creating a list from train messages\n",
    "messages_train = train['new message'].to_numpy()\n",
    "# creating a list from test messages\n",
    "messages_test = test['new message'].to_numpy()\n",
    "\n",
    "# function to remove stopwords from messages\n",
    "def remove_stopwords(messages):\n",
    "    output_array=[]\n",
    "    for sentence in messages:\n",
    "        temp_list=[]\n",
    "        for word in sentence.split():\n",
    "            if word.lower() not in custom_stopwords:\n",
    "                temp_list.append(word)\n",
    "        output_array.append(' '.join(temp_list))\n",
    "    return output_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying stopword funtion to train set\n",
    "train['new message'] = remove_stopwords(messages_train)\n",
    "# applying stopword funtion to test set\n",
    "test['new message'] = remove_stopwords(messages_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>new message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>polyscimajor epa chief doesn't think carbon di...</td>\n",
       "      <td>625221</td>\n",
       "      <td>poly sci major epa chief doesnt think carbon d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>it's not like we lack evidence of anthropogeni...</td>\n",
       "      <td>126103</td>\n",
       "      <td>not like lack evidence anthropogenic global wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>rt  researchers say we have three years to act...</td>\n",
       "      <td>698562</td>\n",
       "      <td>retweet researchers say three years act climat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>todayinmaker  wired   2016 was a pivotal year...</td>\n",
       "      <td>573736</td>\n",
       "      <td>today maker wired 2016 pivotal year war climat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>rt  it's 2016, and a racist, sexist, climate c...</td>\n",
       "      <td>466954</td>\n",
       "      <td>retweet 2016 racist sexist climate change deny...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid  \\\n",
       "0          1  polyscimajor epa chief doesn't think carbon di...   625221   \n",
       "1          1  it's not like we lack evidence of anthropogeni...   126103   \n",
       "2          2  rt  researchers say we have three years to act...   698562   \n",
       "3          1   todayinmaker  wired   2016 was a pivotal year...   573736   \n",
       "4          1  rt  it's 2016, and a racist, sexist, climate c...   466954   \n",
       "\n",
       "                                         new message  \n",
       "0  poly sci major epa chief doesnt think carbon d...  \n",
       "1  not like lack evidence anthropogenic global wa...  \n",
       "2  retweet researchers say three years act climat...  \n",
       "3  today maker wired 2016 pivotal year war climat...  \n",
       "4  retweet 2016 racist sexist climate change deny...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>new message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>europe will now be looking to china to make su...</td>\n",
       "      <td>169760</td>\n",
       "      <td>europe looking china make sure not alone fight...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>combine this with the polling of staffers re c...</td>\n",
       "      <td>35326</td>\n",
       "      <td>combine polling staffers climate change womens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the scary, unimpeachable evidence that climate...</td>\n",
       "      <td>224985</td>\n",
       "      <td>scary unimpeachable evidence climate change al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nputin got to you too jill ! \\ntrump does...</td>\n",
       "      <td>476263</td>\n",
       "      <td>put got jill trump doesnt believe climate chan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rt  'female orgasms cause global warming!'\\n s...</td>\n",
       "      <td>872928</td>\n",
       "      <td>retweet female orgasms cause global warming sa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  tweetid  \\\n",
       "0  europe will now be looking to china to make su...   169760   \n",
       "1  combine this with the polling of staffers re c...    35326   \n",
       "2  the scary, unimpeachable evidence that climate...   224985   \n",
       "3      \\nputin got to you too jill ! \\ntrump does...   476263   \n",
       "4  rt  'female orgasms cause global warming!'\\n s...   872928   \n",
       "\n",
       "                                         new message  \n",
       "0  europe looking china make sure not alone fight...  \n",
       "1  combine polling staffers climate change womens...  \n",
       "2  scary unimpeachable evidence climate change al...  \n",
       "3  put got jill trump doesnt believe climate chan...  \n",
       "4  retweet female orgasms cause global warming sa...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most Frequent Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "climate    13467\n",
       "change     12945\n",
       "retweet     9715\n",
       "global      3884\n",
       "warming     3598\n",
       "s           2866\n",
       "trump       2167\n",
       "believe     1161\n",
       "not         1101\n",
       "t            894\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_freq = pd.Series(' '.join(train['new message']).split()).value_counts()[:10]\n",
    "most_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Less Frequent Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cem             1\n",
       "hawked          1\n",
       "malawi          1\n",
       "ava             1\n",
       "expects         1\n",
       "downpours       1\n",
       "experimented    1\n",
       "deng            1\n",
       "unholy          1\n",
       "ezra            1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "less_freq = pd.Series(' '.join(train['new message']).split()).value_counts()[-10:]\n",
    "less_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer, word_tokenize\n",
    "tokeniser = TreebankWordTokenizer()\n",
    "\n",
    "#tokenizing the train set\n",
    "train['tokens'] = train['new message'].apply(tokeniser.tokenize)\n",
    "#tokenizing the test set\n",
    "test['tokens'] = test['new message'].apply(tokeniser.tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "porter_stemmer = PorterStemmer()\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "\n",
    "# function for stemming\n",
    "def dataset_stem(words, stemmer):\n",
    "    return [snowball_stemmer.stem(word) for word in words]\n",
    "\n",
    "# applying stemming to the train set\n",
    "train['snowball_stemmer'] = train['tokens'].apply(dataset_stem, args=(snowball_stemmer,))\n",
    "train['porter_stemmer'] = train['tokens'].apply(dataset_stem, args=(porter_stemmer,))\n",
    "train['lancaster_stemmer'] = train['tokens'].apply(dataset_stem, args=(lancaster_stemmer,))\n",
    "\n",
    "# applying stemming to the test set\n",
    "test['snowball_stemmer'] = test['tokens'].apply(dataset_stem, args=(snowball_stemmer,))\n",
    "test['porter_stemmer'] = test['tokens'].apply(dataset_stem, args=(porter_stemmer,))\n",
    "test['lancaster_stemmer'] = test['tokens'].apply(dataset_stem, args=(lancaster_stemmer,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function for Lemmatizer\n",
    "def dataset_lemma(words, lemmatizer):\n",
    "    return [lemmatizer.lemmatize(word) for word in words]   \n",
    "\n",
    "#Applying lemmatizer of train set\n",
    "train['lemma'] = train['tokens'].apply(dataset_lemma, args=(lemmatizer, ))\n",
    "\n",
    "#Applying lemmatizer of train set\n",
    "test['lemma'] = test['tokens'].apply(dataset_lemma, args=(lemmatizer, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining words in stemming and Lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining words for the snowball stemming on the train set\n",
    "snowball_stem_sentence_train = []\n",
    "for words in train['snowball_stemmer']:\n",
    "    snowball_stem_sentence_train.append(' '.join(words))\n",
    "\n",
    "# joining words for the porter stemming on the train set\n",
    "porter_stem_sentence_train = []\n",
    "for words in train['porter_stemmer']:\n",
    "    porter_stem_sentence_train.append(' '.join(words))\n",
    "\n",
    "# joining words for the lancaster stemming on the train set\n",
    "lancaster_stem_sentence_train = []\n",
    "for words in train['lancaster_stemmer']:\n",
    "    lancaster_stem_sentence_train.append(' '.join(words))\n",
    "\n",
    "# joining words for the lemma on the train set\n",
    "lemma_sentence_train = []\n",
    "for words in train['lemma']:\n",
    "    lemma_sentence_train.append(' '.join(words))\n",
    "\n",
    "# setting lists to the dataframe column\n",
    "train['snowball_stemmer'] = snowball_stem_sentence_train\n",
    "train['porter_stemmer'] = porter_stem_sentence_train\n",
    "train['lancaster_stemmer'] = lancaster_stem_sentence_train\n",
    "train['lemma'] = lemma_sentence_train\n",
    "    \n",
    "\n",
    "# joining words for the snowball stemming on the test set\n",
    "snowball_stem_sentence_test = []\n",
    "for words in test['snowball_stemmer']:\n",
    "    snowball_stem_sentence_test.append(' '.join(words))\n",
    "\n",
    "# joining words for the porter stemming on the test set\n",
    "porter_stem_sentence_test = []\n",
    "for words in test['porter_stemmer']:\n",
    "    porter_stem_sentence_test.append(' '.join(words))\n",
    "\n",
    "# joining words for the lancaster stemming on the test set\n",
    "lancaster_stem_sentence_test = []\n",
    "for words in test['lancaster_stemmer']:\n",
    "    lancaster_stem_sentence_test.append(' '.join(words))\n",
    "\n",
    "# joining words for the lemma on the test set\n",
    "lemma_sentence_test = []\n",
    "for words in test['lemma']:\n",
    "    lemma_sentence_test.append(' '.join(words))\n",
    "\n",
    "# setting lists to the dataframe column\n",
    "test['snowball_stemmer'] = snowball_stem_sentence_test\n",
    "test['porter_stemmer'] = porter_stem_sentence_test\n",
    "test['lancaster_stemmer'] = lancaster_stem_sentence_test\n",
    "test['lemma'] = lemma_sentence_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>new message</th>\n",
       "      <th>tokens</th>\n",
       "      <th>snowball_stemmer</th>\n",
       "      <th>porter_stemmer</th>\n",
       "      <th>lancaster_stemmer</th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>polyscimajor epa chief doesn't think carbon di...</td>\n",
       "      <td>625221</td>\n",
       "      <td>poly sci major epa chief doesnt think carbon d...</td>\n",
       "      <td>[poly, sci, major, epa, chief, doesnt, think, ...</td>\n",
       "      <td>poli sci major epa chief doesnt think carbon d...</td>\n",
       "      <td>poli sci major epa chief doesnt think carbon d...</td>\n",
       "      <td>poli sci major epa chief doesnt think carbon d...</td>\n",
       "      <td>poly sci major epa chief doesnt think carbon d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>it's not like we lack evidence of anthropogeni...</td>\n",
       "      <td>126103</td>\n",
       "      <td>not like lack evidence anthropogenic global wa...</td>\n",
       "      <td>[not, like, lack, evidence, anthropogenic, glo...</td>\n",
       "      <td>not like lack evid anthropogen global warm</td>\n",
       "      <td>not like lack evid anthropogen global warm</td>\n",
       "      <td>not like lack evid anthropogen global warm</td>\n",
       "      <td>not like lack evidence anthropogenic global wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>rt  researchers say we have three years to act...</td>\n",
       "      <td>698562</td>\n",
       "      <td>retweet researchers say three years act climat...</td>\n",
       "      <td>[retweet, researchers, say, three, years, act,...</td>\n",
       "      <td>retweet research say three year act climat cha...</td>\n",
       "      <td>retweet research say three year act climat cha...</td>\n",
       "      <td>retweet research say three year act climat cha...</td>\n",
       "      <td>retweet researcher say three year act climate ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>todayinmaker  wired   2016 was a pivotal year...</td>\n",
       "      <td>573736</td>\n",
       "      <td>today maker wired 2016 pivotal year war climat...</td>\n",
       "      <td>[today, maker, wired, 2016, pivotal, year, war...</td>\n",
       "      <td>today maker wire 2016 pivot year war climat chang</td>\n",
       "      <td>today maker wire 2016 pivot year war climat chang</td>\n",
       "      <td>today maker wire 2016 pivot year war climat chang</td>\n",
       "      <td>today maker wired 2016 pivotal year war climat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>rt  it's 2016, and a racist, sexist, climate c...</td>\n",
       "      <td>466954</td>\n",
       "      <td>retweet 2016 racist sexist climate change deny...</td>\n",
       "      <td>[retweet, 2016, racist, sexist, climate, chang...</td>\n",
       "      <td>retweet 2016 racist sexist climat chang deni b...</td>\n",
       "      <td>retweet 2016 racist sexist climat chang deni b...</td>\n",
       "      <td>retweet 2016 racist sexist climat chang deni b...</td>\n",
       "      <td>retweet 2016 racist sexist climate change deny...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid  \\\n",
       "0          1  polyscimajor epa chief doesn't think carbon di...   625221   \n",
       "1          1  it's not like we lack evidence of anthropogeni...   126103   \n",
       "2          2  rt  researchers say we have three years to act...   698562   \n",
       "3          1   todayinmaker  wired   2016 was a pivotal year...   573736   \n",
       "4          1  rt  it's 2016, and a racist, sexist, climate c...   466954   \n",
       "\n",
       "                                         new message  \\\n",
       "0  poly sci major epa chief doesnt think carbon d...   \n",
       "1  not like lack evidence anthropogenic global wa...   \n",
       "2  retweet researchers say three years act climat...   \n",
       "3  today maker wired 2016 pivotal year war climat...   \n",
       "4  retweet 2016 racist sexist climate change deny...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [poly, sci, major, epa, chief, doesnt, think, ...   \n",
       "1  [not, like, lack, evidence, anthropogenic, glo...   \n",
       "2  [retweet, researchers, say, three, years, act,...   \n",
       "3  [today, maker, wired, 2016, pivotal, year, war...   \n",
       "4  [retweet, 2016, racist, sexist, climate, chang...   \n",
       "\n",
       "                                    snowball_stemmer  \\\n",
       "0  poli sci major epa chief doesnt think carbon d...   \n",
       "1         not like lack evid anthropogen global warm   \n",
       "2  retweet research say three year act climat cha...   \n",
       "3  today maker wire 2016 pivot year war climat chang   \n",
       "4  retweet 2016 racist sexist climat chang deni b...   \n",
       "\n",
       "                                      porter_stemmer  \\\n",
       "0  poli sci major epa chief doesnt think carbon d...   \n",
       "1         not like lack evid anthropogen global warm   \n",
       "2  retweet research say three year act climat cha...   \n",
       "3  today maker wire 2016 pivot year war climat chang   \n",
       "4  retweet 2016 racist sexist climat chang deni b...   \n",
       "\n",
       "                                   lancaster_stemmer  \\\n",
       "0  poli sci major epa chief doesnt think carbon d...   \n",
       "1         not like lack evid anthropogen global warm   \n",
       "2  retweet research say three year act climat cha...   \n",
       "3  today maker wire 2016 pivot year war climat chang   \n",
       "4  retweet 2016 racist sexist climat chang deni b...   \n",
       "\n",
       "                                               lemma  \n",
       "0  poly sci major epa chief doesnt think carbon d...  \n",
       "1  not like lack evidence anthropogenic global wa...  \n",
       "2  retweet researcher say three year act climate ...  \n",
       "3  today maker wired 2016 pivotal year war climat...  \n",
       "4  retweet 2016 racist sexist climate change deny...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>new message</th>\n",
       "      <th>tokens</th>\n",
       "      <th>snowball_stemmer</th>\n",
       "      <th>porter_stemmer</th>\n",
       "      <th>lancaster_stemmer</th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>europe will now be looking to china to make su...</td>\n",
       "      <td>169760</td>\n",
       "      <td>europe looking china make sure not alone fight...</td>\n",
       "      <td>[europe, looking, china, make, sure, not, alon...</td>\n",
       "      <td>europ look china make sure not alon fight clim...</td>\n",
       "      <td>europ look china make sure not alon fight clim...</td>\n",
       "      <td>europ look china make sure not alon fight clim...</td>\n",
       "      <td>europe looking china make sure not alone fight...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>combine this with the polling of staffers re c...</td>\n",
       "      <td>35326</td>\n",
       "      <td>combine polling staffers climate change womens...</td>\n",
       "      <td>[combine, polling, staffers, climate, change, ...</td>\n",
       "      <td>combin poll staffer climat chang women right f...</td>\n",
       "      <td>combin poll staffer climat chang women right f...</td>\n",
       "      <td>combin poll staffer climat chang women right f...</td>\n",
       "      <td>combine polling staffer climate change woman r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the scary, unimpeachable evidence that climate...</td>\n",
       "      <td>224985</td>\n",
       "      <td>scary unimpeachable evidence climate change al...</td>\n",
       "      <td>[scary, unimpeachable, evidence, climate, chan...</td>\n",
       "      <td>scari unimpeach evid climat chang alreadi time...</td>\n",
       "      <td>scari unimpeach evid climat chang alreadi time...</td>\n",
       "      <td>scari unimpeach evid climat chang alreadi time...</td>\n",
       "      <td>scary unimpeachable evidence climate change al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nputin got to you too jill ! \\ntrump does...</td>\n",
       "      <td>476263</td>\n",
       "      <td>put got jill trump doesnt believe climate chan...</td>\n",
       "      <td>[put, got, jill, trump, doesnt, believe, clima...</td>\n",
       "      <td>put got jill trump doesnt believ climat chang ...</td>\n",
       "      <td>put got jill trump doesnt believ climat chang ...</td>\n",
       "      <td>put got jill trump doesnt believ climat chang ...</td>\n",
       "      <td>put got jill trump doesnt believe climate chan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rt  'female orgasms cause global warming!'\\n s...</td>\n",
       "      <td>872928</td>\n",
       "      <td>retweet female orgasms cause global warming sa...</td>\n",
       "      <td>[retweet, female, orgasms, cause, global, warm...</td>\n",
       "      <td>retweet femal orgasm caus global warm sarcast ...</td>\n",
       "      <td>retweet femal orgasm caus global warm sarcast ...</td>\n",
       "      <td>retweet femal orgasm caus global warm sarcast ...</td>\n",
       "      <td>retweet female orgasm cause global warming sar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  tweetid  \\\n",
       "0  europe will now be looking to china to make su...   169760   \n",
       "1  combine this with the polling of staffers re c...    35326   \n",
       "2  the scary, unimpeachable evidence that climate...   224985   \n",
       "3      \\nputin got to you too jill ! \\ntrump does...   476263   \n",
       "4  rt  'female orgasms cause global warming!'\\n s...   872928   \n",
       "\n",
       "                                         new message  \\\n",
       "0  europe looking china make sure not alone fight...   \n",
       "1  combine polling staffers climate change womens...   \n",
       "2  scary unimpeachable evidence climate change al...   \n",
       "3  put got jill trump doesnt believe climate chan...   \n",
       "4  retweet female orgasms cause global warming sa...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [europe, looking, china, make, sure, not, alon...   \n",
       "1  [combine, polling, staffers, climate, change, ...   \n",
       "2  [scary, unimpeachable, evidence, climate, chan...   \n",
       "3  [put, got, jill, trump, doesnt, believe, clima...   \n",
       "4  [retweet, female, orgasms, cause, global, warm...   \n",
       "\n",
       "                                    snowball_stemmer  \\\n",
       "0  europ look china make sure not alon fight clim...   \n",
       "1  combin poll staffer climat chang women right f...   \n",
       "2  scari unimpeach evid climat chang alreadi time...   \n",
       "3  put got jill trump doesnt believ climat chang ...   \n",
       "4  retweet femal orgasm caus global warm sarcast ...   \n",
       "\n",
       "                                      porter_stemmer  \\\n",
       "0  europ look china make sure not alon fight clim...   \n",
       "1  combin poll staffer climat chang women right f...   \n",
       "2  scari unimpeach evid climat chang alreadi time...   \n",
       "3  put got jill trump doesnt believ climat chang ...   \n",
       "4  retweet femal orgasm caus global warm sarcast ...   \n",
       "\n",
       "                                   lancaster_stemmer  \\\n",
       "0  europ look china make sure not alon fight clim...   \n",
       "1  combin poll staffer climat chang women right f...   \n",
       "2  scari unimpeach evid climat chang alreadi time...   \n",
       "3  put got jill trump doesnt believ climat chang ...   \n",
       "4  retweet femal orgasm caus global warm sarcast ...   \n",
       "\n",
       "                                               lemma  \n",
       "0  europe looking china make sure not alone fight...  \n",
       "1  combine polling staffer climate change woman r...  \n",
       "2  scary unimpeachable evidence climate change al...  \n",
       "3  put got jill trump doesnt believe climate chan...  \n",
       "4  retweet female orgasm cause global warming sar...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "polarity = []\n",
    "subjectivity = []\n",
    "for sentence in train['lemma']:\n",
    "    polarity.append((TextBlob(sentence).sentiment.polarity))\n",
    "    subjectivity.append((TextBlob(sentence).sentiment.subjectivity))\n",
    "    \n",
    "train['polarity'] = polarity\n",
    "train['subjectivity'] = subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>new message</th>\n",
       "      <th>tokens</th>\n",
       "      <th>snowball_stemmer</th>\n",
       "      <th>porter_stemmer</th>\n",
       "      <th>lancaster_stemmer</th>\n",
       "      <th>lemma</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>polyscimajor epa chief doesn't think carbon di...</td>\n",
       "      <td>625221</td>\n",
       "      <td>poly sci major epa chief doesnt think carbon d...</td>\n",
       "      <td>[poly, sci, major, epa, chief, doesnt, think, ...</td>\n",
       "      <td>poli sci major epa chief doesnt think carbon d...</td>\n",
       "      <td>poli sci major epa chief doesnt think carbon d...</td>\n",
       "      <td>poli sci major epa chief doesnt think carbon d...</td>\n",
       "      <td>poly sci major epa chief doesnt think carbon d...</td>\n",
       "      <td>0.076389</td>\n",
       "      <td>0.277778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>it's not like we lack evidence of anthropogeni...</td>\n",
       "      <td>126103</td>\n",
       "      <td>not like lack evidence anthropogenic global wa...</td>\n",
       "      <td>[not, like, lack, evidence, anthropogenic, glo...</td>\n",
       "      <td>not like lack evid anthropogen global warm</td>\n",
       "      <td>not like lack evid anthropogen global warm</td>\n",
       "      <td>not like lack evid anthropogen global warm</td>\n",
       "      <td>not like lack evidence anthropogenic global wa...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>rt  researchers say we have three years to act...</td>\n",
       "      <td>698562</td>\n",
       "      <td>retweet researchers say three years act climat...</td>\n",
       "      <td>[retweet, researchers, say, three, years, act,...</td>\n",
       "      <td>retweet research say three year act climat cha...</td>\n",
       "      <td>retweet research say three year act climat cha...</td>\n",
       "      <td>retweet research say three year act climat cha...</td>\n",
       "      <td>retweet researcher say three year act climate ...</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>todayinmaker  wired   2016 was a pivotal year...</td>\n",
       "      <td>573736</td>\n",
       "      <td>today maker wired 2016 pivotal year war climat...</td>\n",
       "      <td>[today, maker, wired, 2016, pivotal, year, war...</td>\n",
       "      <td>today maker wire 2016 pivot year war climat chang</td>\n",
       "      <td>today maker wire 2016 pivot year war climat chang</td>\n",
       "      <td>today maker wire 2016 pivot year war climat chang</td>\n",
       "      <td>today maker wired 2016 pivotal year war climat...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>rt  it's 2016, and a racist, sexist, climate c...</td>\n",
       "      <td>466954</td>\n",
       "      <td>retweet 2016 racist sexist climate change deny...</td>\n",
       "      <td>[retweet, 2016, racist, sexist, climate, chang...</td>\n",
       "      <td>retweet 2016 racist sexist climat chang deni b...</td>\n",
       "      <td>retweet 2016 racist sexist climat chang deni b...</td>\n",
       "      <td>retweet 2016 racist sexist climat chang deni b...</td>\n",
       "      <td>retweet 2016 racist sexist climate change deny...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid  \\\n",
       "0          1  polyscimajor epa chief doesn't think carbon di...   625221   \n",
       "1          1  it's not like we lack evidence of anthropogeni...   126103   \n",
       "2          2  rt  researchers say we have three years to act...   698562   \n",
       "3          1   todayinmaker  wired   2016 was a pivotal year...   573736   \n",
       "4          1  rt  it's 2016, and a racist, sexist, climate c...   466954   \n",
       "\n",
       "                                         new message  \\\n",
       "0  poly sci major epa chief doesnt think carbon d...   \n",
       "1  not like lack evidence anthropogenic global wa...   \n",
       "2  retweet researchers say three years act climat...   \n",
       "3  today maker wired 2016 pivotal year war climat...   \n",
       "4  retweet 2016 racist sexist climate change deny...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [poly, sci, major, epa, chief, doesnt, think, ...   \n",
       "1  [not, like, lack, evidence, anthropogenic, glo...   \n",
       "2  [retweet, researchers, say, three, years, act,...   \n",
       "3  [today, maker, wired, 2016, pivotal, year, war...   \n",
       "4  [retweet, 2016, racist, sexist, climate, chang...   \n",
       "\n",
       "                                    snowball_stemmer  \\\n",
       "0  poli sci major epa chief doesnt think carbon d...   \n",
       "1         not like lack evid anthropogen global warm   \n",
       "2  retweet research say three year act climat cha...   \n",
       "3  today maker wire 2016 pivot year war climat chang   \n",
       "4  retweet 2016 racist sexist climat chang deni b...   \n",
       "\n",
       "                                      porter_stemmer  \\\n",
       "0  poli sci major epa chief doesnt think carbon d...   \n",
       "1         not like lack evid anthropogen global warm   \n",
       "2  retweet research say three year act climat cha...   \n",
       "3  today maker wire 2016 pivot year war climat chang   \n",
       "4  retweet 2016 racist sexist climat chang deni b...   \n",
       "\n",
       "                                   lancaster_stemmer  \\\n",
       "0  poli sci major epa chief doesnt think carbon d...   \n",
       "1         not like lack evid anthropogen global warm   \n",
       "2  retweet research say three year act climat cha...   \n",
       "3  today maker wire 2016 pivot year war climat chang   \n",
       "4  retweet 2016 racist sexist climat chang deni b...   \n",
       "\n",
       "                                               lemma  polarity  subjectivity  \n",
       "0  poly sci major epa chief doesnt think carbon d...  0.076389      0.277778  \n",
       "1  not like lack evidence anthropogenic global wa...  0.000000      0.000000  \n",
       "2  retweet researcher say three year act climate ... -0.300000      0.600000  \n",
       "3  today maker wired 2016 pivotal year war climat...  0.500000      0.800000  \n",
       "4  retweet 2016 racist sexist climate change deny...  0.000000      0.000000  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017369,
     "end_time": "2020-10-07T10:31:44.053776",
     "exception": false,
     "start_time": "2020-10-07T10:31:44.036407",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Splitting out the X variable from the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-07T10:31:44.095546Z",
     "iopub.status.busy": "2020-10-07T10:31:44.094614Z",
     "iopub.status.idle": "2020-10-07T10:31:44.097366Z",
     "shell.execute_reply": "2020-10-07T10:31:44.097898Z"
    },
    "papermill": {
     "duration": 0.026517,
     "end_time": "2020-10-07T10:31:44.098066",
     "exception": false,
     "start_time": "2020-10-07T10:31:44.071549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = train['sentiment']\n",
    "X = train['lemma']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017532,
     "end_time": "2020-10-07T10:31:44.133706",
     "exception": false,
     "start_time": "2020-10-07T10:31:44.116174",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Turning text into something your model can read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-07T10:31:44.204236Z",
     "iopub.status.busy": "2020-10-07T10:31:44.193790Z",
     "iopub.status.idle": "2020-10-07T10:31:45.254842Z",
     "shell.execute_reply": "2020-10-07T10:31:45.254115Z"
    },
    "papermill": {
     "duration": 1.103354,
     "end_time": "2020-10-07T10:31:45.255019",
     "exception": false,
     "start_time": "2020-10-07T10:31:44.151665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df=2)\n",
    "X_vectorized = vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017958,
     "end_time": "2020-10-07T10:31:45.291412",
     "exception": false,
     "start_time": "2020-10-07T10:31:45.273454",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Splitting the training data into a training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-07T10:31:45.334599Z",
     "iopub.status.busy": "2020-10-07T10:31:45.333798Z",
     "iopub.status.idle": "2020-10-07T10:31:45.354945Z",
     "shell.execute_reply": "2020-10-07T10:31:45.354123Z"
    },
    "papermill": {
     "duration": 0.045495,
     "end_time": "2020-10-07T10:31:45.355113",
     "exception": false,
     "start_time": "2020-10-07T10:31:45.309618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train,X_val,y_train,y_val = train_test_split(X_vectorized,y,test_size=0.1, random_state=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018099,
     "end_time": "2020-10-07T10:31:45.391807",
     "exception": false,
     "start_time": "2020-10-07T10:31:45.373708",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Creating a function to measure best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "def model_selection(model, X_train, X_val, y_train, y_val):\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    return print(f1_score(y_val, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6869158097766954\n"
     ]
    }
   ],
   "source": [
    "rfc = LinearSVC(dual= True,fit_intercept= False,loss= 'hinge',multi_class= 'ovr',penalty= 'l2')\n",
    "model_selection(rfc, X_train, X_val, y_train, y_val)\n",
    "\n",
    "#0.6627376887718992"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-07T10:31:45.434997Z",
     "iopub.status.busy": "2020-10-07T10:31:45.433825Z",
     "iopub.status.idle": "2020-10-07T10:32:13.157726Z",
     "shell.execute_reply": "2020-10-07T10:32:13.156924Z"
    },
    "papermill": {
     "duration": 27.747703,
     "end_time": "2020-10-07T10:32:13.157852",
     "exception": false,
     "start_time": "2020-10-07T10:31:45.410149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6268069636564937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NL3005\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "model_selection(LogisticRegression(), X_train, X_val, y_train, y_val)\n",
    "\n",
    "#0.619999147722196"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6028702705628369\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier()\n",
    "model_selection(XGBClassifier(), X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=False,\n",
       "          intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
       "          penalty='l2', random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "rfc = LinearSVC(dual= True,fit_intercept= False,loss= 'hinge',multi_class= 'ovr',penalty= 'l2')\n",
    "rfc.fit(X_vectorized, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>tokens</th>\n",
       "      <th>snowball_stemmer</th>\n",
       "      <th>porter_stemmer</th>\n",
       "      <th>lancaster_stemmer</th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>europe looking china make sure fighting climat...</td>\n",
       "      <td>169760</td>\n",
       "      <td>[europe, looking, china, make, sure, fighting,...</td>\n",
       "      <td>europ look china make sure fight climat chang</td>\n",
       "      <td>europ look china make sure fight climat chang</td>\n",
       "      <td>europ look china make sure fight climat chang</td>\n",
       "      <td>europe looking china make sure fighting climat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>combine polling staffers climate change womens...</td>\n",
       "      <td>35326</td>\n",
       "      <td>[combine, polling, staffers, climate, change, ...</td>\n",
       "      <td>combin poll staffer climat chang women right f...</td>\n",
       "      <td>combin poll staffer climat chang women right f...</td>\n",
       "      <td>combin poll staffer climat chang women right f...</td>\n",
       "      <td>combine polling staffer climate change woman r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>scary unimpeachable evidence climate change ti...</td>\n",
       "      <td>224985</td>\n",
       "      <td>[scary, unimpeachable, evidence, climate, chan...</td>\n",
       "      <td>scari unimpeach evid climat chang time chang c...</td>\n",
       "      <td>scari unimpeach evid climat chang time chang c...</td>\n",
       "      <td>scari unimpeach evid climat chang time chang c...</td>\n",
       "      <td>scary unimpeachable evidence climate change ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>got jill trump doesnt believe climate change t...</td>\n",
       "      <td>476263</td>\n",
       "      <td>[got, jill, trump, doesnt, believe, climate, c...</td>\n",
       "      <td>got jill trump doesnt believ climat chang thin...</td>\n",
       "      <td>got jill trump doesnt believ climat chang thin...</td>\n",
       "      <td>got jill trump doesnt believ climat chang thin...</td>\n",
       "      <td>got jill trump doesnt believe climate change t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>retweet female orgasms cause global warming sa...</td>\n",
       "      <td>872928</td>\n",
       "      <td>[retweet, female, orgasms, cause, global, warm...</td>\n",
       "      <td>retweet femal orgasm caus global warm sarcast ...</td>\n",
       "      <td>retweet femal orgasm caus global warm sarcast ...</td>\n",
       "      <td>retweet femal orgasm caus global warm sarcast ...</td>\n",
       "      <td>retweet female orgasm cause global warming sar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  tweetid  \\\n",
       "0  europe looking china make sure fighting climat...   169760   \n",
       "1  combine polling staffers climate change womens...    35326   \n",
       "2  scary unimpeachable evidence climate change ti...   224985   \n",
       "3  got jill trump doesnt believe climate change t...   476263   \n",
       "4  retweet female orgasms cause global warming sa...   872928   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [europe, looking, china, make, sure, fighting,...   \n",
       "1  [combine, polling, staffers, climate, change, ...   \n",
       "2  [scary, unimpeachable, evidence, climate, chan...   \n",
       "3  [got, jill, trump, doesnt, believe, climate, c...   \n",
       "4  [retweet, female, orgasms, cause, global, warm...   \n",
       "\n",
       "                                    snowball_stemmer  \\\n",
       "0      europ look china make sure fight climat chang   \n",
       "1  combin poll staffer climat chang women right f...   \n",
       "2  scari unimpeach evid climat chang time chang c...   \n",
       "3  got jill trump doesnt believ climat chang thin...   \n",
       "4  retweet femal orgasm caus global warm sarcast ...   \n",
       "\n",
       "                                      porter_stemmer  \\\n",
       "0      europ look china make sure fight climat chang   \n",
       "1  combin poll staffer climat chang women right f...   \n",
       "2  scari unimpeach evid climat chang time chang c...   \n",
       "3  got jill trump doesnt believ climat chang thin...   \n",
       "4  retweet femal orgasm caus global warm sarcast ...   \n",
       "\n",
       "                                   lancaster_stemmer  \\\n",
       "0      europ look china make sure fight climat chang   \n",
       "1  combin poll staffer climat chang women right f...   \n",
       "2  scari unimpeach evid climat chang time chang c...   \n",
       "3  got jill trump doesnt believ climat chang thin...   \n",
       "4  retweet femal orgasm caus global warm sarcast ...   \n",
       "\n",
       "                                               lemma  \n",
       "0  europe looking china make sure fighting climat...  \n",
       "1  combine polling staffer climate change woman r...  \n",
       "2  scary unimpeachable evidence climate change ti...  \n",
       "3  got jill trump doesnt believe climate change t...  \n",
       "4  retweet female orgasm cause global warming sar...  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018914,
     "end_time": "2020-10-07T10:32:13.283518",
     "exception": false,
     "start_time": "2020-10-07T10:32:13.264604",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Getting our test set ready "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-07T10:32:13.357774Z",
     "iopub.status.busy": "2020-10-07T10:32:13.347375Z",
     "iopub.status.idle": "2020-10-07T10:32:13.858822Z",
     "shell.execute_reply": "2020-10-07T10:32:13.858192Z"
    },
    "papermill": {
     "duration": 0.556389,
     "end_time": "2020-10-07T10:32:13.858972",
     "exception": false,
     "start_time": "2020-10-07T10:32:13.302583",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "testx = test['lemma']\n",
    "test_vect = vectorizer.transform(testx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01875,
     "end_time": "2020-10-07T10:32:13.896971",
     "exception": false,
     "start_time": "2020-10-07T10:32:13.878221",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Making predictions on the test set and adding a sentiment column to our original test df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-07T10:32:13.944114Z",
     "iopub.status.busy": "2020-10-07T10:32:13.943062Z",
     "iopub.status.idle": "2020-10-07T10:32:14.775936Z",
     "shell.execute_reply": "2020-10-07T10:32:14.775252Z"
    },
    "papermill": {
     "duration": 0.859158,
     "end_time": "2020-10-07T10:32:14.776067",
     "exception": false,
     "start_time": "2020-10-07T10:32:13.916909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = rfc.predict(test_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-07T10:32:14.822256Z",
     "iopub.status.busy": "2020-10-07T10:32:14.821170Z",
     "iopub.status.idle": "2020-10-07T10:32:14.824844Z",
     "shell.execute_reply": "2020-10-07T10:32:14.824131Z"
    },
    "papermill": {
     "duration": 0.029243,
     "end_time": "2020-10-07T10:32:14.824969",
     "exception": false,
     "start_time": "2020-10-07T10:32:14.795726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test['sentiment'] = y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.020114,
     "end_time": "2020-10-07T10:32:14.923436",
     "exception": false,
     "start_time": "2020-10-07T10:32:14.903322",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Creating an output csv for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-07T10:32:14.974377Z",
     "iopub.status.busy": "2020-10-07T10:32:14.970165Z",
     "iopub.status.idle": "2020-10-07T10:32:15.292997Z",
     "shell.execute_reply": "2020-10-07T10:32:15.292333Z"
    },
    "papermill": {
     "duration": 0.349338,
     "end_time": "2020-10-07T10:32:15.293155",
     "exception": false,
     "start_time": "2020-10-07T10:32:14.943817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test[['tweetid','sentiment']].to_csv('testsubmission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 38.75415,
   "end_time": "2020-10-07T10:32:15.462679",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-10-07T10:31:36.708529",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
